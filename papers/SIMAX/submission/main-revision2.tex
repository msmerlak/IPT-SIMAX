% SIAM Article Template
\documentclass[final,onefignum,onetabnum]{siamart190516}
\usepackage{caption}
% Information that is shared between the article and the supplement
% (title and author information, macros, packages, etc.) goes into
% ex_shared.tex. If there is no supplement, this file can be included
% directly.

\input{shared}

% Optional PDF information
%\ifpdf
%\hypersetup{
%  pdftitle={An Example Article},
%  pdfauthor={D. Doe, P. T. Frank, and J. E. Smith}
%}
%\fi

% The next statement enables references to information in the
% supplement. See the xr-hyperref package for details.

\newcommand{\D}{\partial}

% Commands for a quick switch of notations
\newcommand{\Z}{Z} % the matrix of eigenvectors
\newcommand{\eigval}{\lambda} % the notation for a eigenvalue
\newcommand{\Eigval}{\Lambda} % the notation for a matrix of eigenvalues
\newcommand{\parameter}{\varepsilon} % the notation for the perturbation parameter
\newcommand{\evzero}{\mathring{\lambda}} % the notation for unperturbed eigenvalues (the diagonal elements of D)

% FundRef data to be entered by SIAM
%<funding-group specific-use="FundRef">
%<award-group>
%<funding-source>
%<named-content content-type="funder-name"> 
%</named-content> 
%<named-content content-type="funder-identifier"> 
%</named-content>
%</funding-source>
%<award-id> </award-id>
%</award-group>
%</funding-group>

%\usepackage{algorithmicx}
%\usepackage{algorithm}
%\usepackage{algpseudocode}
\begin{document}

\maketitle

% REQUIRED
\begin{abstract}
We introduce a novel eigenvalue algorithm for near-diagonal matrices termed \textit{iterative perturbative theory} (IPT). Contrary to standard methods which are , IPT can be used to compute one, few, or all eigenpairs with the same basic iterative algorithm. 


Thanks to the high parallelism of matrix-matrix products, full-spectrum IPT shows excellent performance on multi-core processors and GPUs, with large speed-ups over standard direct methods (up to $\sim50$x with respect to LAPACK and MAGMA). When used to compute just one eigenpair, IPT is competitive with Davidson-type methods, without the need for subspace expansion, orthogonlization, or diagonalization in a subspace. For matrices which are not close to being diagonal but have well-separated eigenvalues, IPT can be be used to refine low-precision eigenpairs obtained by other methods. We give sufficient conditions for linear convergence and demonstrate performance on dense and sparse test matrices, including one from quantum chemistry.
\end{abstract}

% REQUIRED
\begin{keywords}
eigenvalue algorithm, perturbation theory, iterative method
\end{keywords}

% REQUIRED
\begin{AMS}
  65F15
\end{AMS}

\section{Introduction}

Computing the eigenvalues and eigenvectors of a matrix that is already close to being diagonal (or diagonalizable in a known basis) is a classic problem with applications throughout science. \textit{Ab initio} studies of electronic and nuclear structures, for instance, often involve Hamiltonian operators expressed in bases that nearly diagonalize them,\textit{ e.g.} in self-consistent field or configuration interaction calculations \cite{ostlund1996}. In classical electromagnetism, perturbative problems arise when the solution to a slightly deformed problem are sought, for instance when boundary conditions are shifted or optical properties are varied \cite{pozar1998}. In mechanical engineering, one often wants to know the effect of slight deformations of stiffness or density on spectra, as in Rayleigh's pioneering analysis of vibrating strings \cite{rayleigh1894}. More generally, we might know approximate eigenpairs through some method, and the problem is to refine them to a given precision. 

Eigenvalue perturbation theory provides estimates and bounds for the variation of eigenpairs with respect to perturbations of the matrix \cite{sun1990}. Here our aim is, instead, to compute these eigenpairs exactly (up to numerical error), \textit{i.e.} we are interested in building an eigenvalue algorithm suitable for perturbative problems. We introduce a new method, termed Iterative Perturbation Theory (IPT) and inspired from the classical Rayleigh-Schr\"odinger perturbation theory, for solving this problem. 

Eigenvalue algorithms are usually classified into `direct' and `iterative' methods, corresponding to the problem of computing all or few eigenpairs, respectively \cite{demmel1997}. Our algorithm makes no such distinction: using the same iterative algorithm we compute any desired number of eigenpairs, from one to all of them (provided they are well separated and the matrix is near-diagonal, see below). We do this by solving a non-linear vector equation per desired eigenpair. Remarkably, each of these equations has \emph{all} eigenvectors as solutions; however, under fixed-point iteration, each converges to a different eigenvector.  

With the exception of Jacobi's algorithm for symmetric matrices, direct methods all start by reducing the matrix into Hessenberg (or tri-diagonal) form. This operation has $\mathcal{O}(n^3)$ complexity, parallelizes poorly, and breaks  sparsity. To compute the full spectrum of a matrix, IPT relies on no such reduction; instead it consists in repeated matrix-matrix multiplications, with sub-cubic theoretical complexity $\mathcal{O}(n^{2.376})$ and highly parallelizable with BLAS 3 operations. We show below that, within its domain of application, IPT outperforms reduction-based direct algorithms by up to two orders of magnitude, especially when implemented on graphical processing units (GPUs) with massively parallel BLAS 3 operations. We also find that IPT tends to be faster than Jacobi's algorithm, whose local quadratic convergence and good parallelizability makes is a natural competitor for near-diagonal problems in spite of its unfavorable theoretical complexity \cite{demmel1997}. 

In cases where only a single (or a few) eigenpairs is desired, IPT provides an alternative to standard iterative algorithms. Like the power method from which they derive, these methods all share a basic structure: they consist in applying the Rayleigh-Ritz method in a small but increasingly adapted subspace. Some of these methods grow this subspace at each iteration, either in a Krylov way (Lanczos, Arnoldi) or using preconditioned residuals (Davidson and Jacobi-Davidson \cite{Davidson_1975, fokkema1998jacobi, sleijpen2000jacobi, Morgan_2009}); others use a small subspace of fixed size, possibly with preconditioning (LOBPCG \cite{knyazev2017recent}). Of these methods, the most efficient for near-diagonal problems tend to be Davidson methods. 

Unlike these iterative methods, IPT is not based on the Rayleigh-Ritz method: it is fully explicit and self-contained, which means that IPT generalizes to infinite-dimensional operators \cite{smerlak2021convergence}. More importantly perhaps, IPT is an elementary fixed-point iteration. This means that the vast repertoire of methods developed in numerical for solving non-linear equations, including fixed-point acceleration methods, become directly relevant to the eigenvalue problem. We feel that this connection between non-linear analysis and numerical linear algebra is the most appealing aspect of IPT.

Iterative perturbation theory (IPT) was inspired by the Rayleigh-Schr\"odinger (RS) perturbative expansion from textbook quantum mechanics \cite{d1965}. (The relationship between the two methods in the context of quantum physics is discussed more thoroughly in a companion paper \cite{smerlak2021convergence}.) Its structure, however, is different: instead of seeking the perturbed eigenvectors as power series in the perturbation, we compute them iteratively, as fixed points of a quadratic equation. One consequence of this difference is that, unlike Rayleigh-Schr\"odinger expansions \cite{kato1995}, the domain of convergence of our method as a scalar perturbation parameter is varied is not restricted to a disk in the complex plane bounded by exceptional points (values of the parameter such that the matrix is defective); instead, IPT follows for large perturbations the period-doubling route to chaos. In many cases, this allows for a larger convergence domain than RS perturbation theory.

Our presentation starts with a reformulation of the eigenvalue equation as a fixed point equation for a quadratic map in complex projective space (\cref{fixed-points}). We then establish a sufficient condition for fixed point iteration to converge and illustrate its divergence for larger perturbations with a simple two-dimensional example (\cref{convergence}). Next, we consider the computational efficiency of our method on multi-CPU and GPU architectures using test matrices, either sampled at random or drawn from chemistry applications (\cref{performance}). We then discuss one possible applications of our algorithm as a refinement method for eigenvectors (\cref{applications}). We conclude with some possible directions for future work.

\section{Definitions and notations}\label{definitions}

Below we denote complex vectors and functions to vector spaces by bold face lowercase letters: $\mathbf v \in \mathbb C^n$. We denote coordinates of vector $\mathbf v$ in the standard basis of $\mathbb C^n$ either by $(\mathbf{v})_i$ or by $v_i$, using the same letter but in Roman. For matrices and functions to spaces of matrices we use upper case Roman letters. Superscripts in parentheses (e.g. $x^{(k)}$) denote the order of an approximation, while the superscripts in square brackets (e.g. $x^{[k]}$) enumerate terms in asymptotic series.

Everywhere in the text we understand by \emph{genericity} of an object in some set $\mathbb C^n$ with the standard topology the condition that that object belongs to a fixed, open, everywhere dense subset of $\mathbb C^n$. A phrase ``condition $A$ is false for a generic object $O \in S$'' should be read as ``condition $A$ corresponds to a closed nowhere dense subset of $S$''. For example, genericity of an $n\times n$ matrix $M$ is understood with $M$ viewed as an element of $\mathbb C^{n\times n}$. A phrase ``eigenvalues of a generic $n\times n$ matrix are pairwise different'' should be understood as ``the set of matrices with eigenvalues of higher multiplicity form a nowhere dense closed subset in $\mathbb C^{n\times n}$''. Genericity of a family of partitions (defined below) $D + \parameter \Delta$, where $D = \diag(\mathbf{v})$, $\mathbf{v} \in \mathbb C^n$, and $\Delta \in \mathbb C^{n\times n}$, $\parameter \in \mathbb C$, is understood as genericity of a point $(\mathbf v,\Delta,\parameter) \in \mathbb C^n \times \mathbb C^{n\times n} \times \mathbb C$, and so on.

We use some elementary notions from projective and differential geometry. Their definitions are given below.

Informally, the $(n-1)$-dimensional projective space $\mathbb{C}P^{n-1}$ is the space of complex lines (1-dimensional complex subspaces) of the vector space $\mathbb{C}^n$ (the notion is generalizable to vectors spaces over fields different from $\mathbb{C}$). It is constructed in the following way. Consider an equivalence relation of \emph{nonzero} vectors $\mathbf{v},\mathbf{w}\in\mathbb{C}^n$ given by $\mathbf{v} \sim \mathbf{w}$ if $\exists \alpha \in \mathbb{C}$ such that $\mathbf{v} = \alpha \mathbf{w}$. In other words, all collinear vectors, with exception of $0$, are declared to be equivalent.

\begin{definition}
The \emph{projective space} $\mathbb{C}P^{n-1}$ is the factor-space $(\mathbb{C}^n\setminus\{0\})/{\sim}$ by the indicated equivalence relation with the associated factor-topology.
\end{definition}

We care about eigenvectors only up to normalization, therefore the projective space is a natural space that contains them as its points. As usual, we denote the equivalence class of a vector $\mathbf{z}\in\mathbb{C}^n$ (a point in $\mathbb{C}P^{n-1}$) as $[\mathbf{z}]$, and $\mathbf{z}$ is called a representative of that class. The coordinates of $\mathbb{C}^n$ can be used to (nonuniquely) parametrize points of $\mathbb{C}P^{n-1}$ in the following way.

\begin{definition}
A tuple $[z_1:z_2:\dots:z_n]$ is called \emph{homogeneous coordinates} of a point $p\in\mathbb{C}P^{n-1}$ if there is a vector $\mathbf{z}\in\mathbb{C}^n$ such that $p = [\mathbf{z}]$ and $\mathbf{z}$ has coordinates $(z_1,z_2,\ldots,z_n)$.
\end{definition}

Thus each point of the projective space has a set of homogeneous coordinates, but all of them are related by homogeneous rescaling: two homogeneous coordinates of the same point $[x_1:\dots:x_n]$ and $[y_1:\dots:y_n]$ are related by $x_i = \alpha y_i$ with the same $\alpha$ for each $i$.

$\mathbb{C}P^{n-1}$ has a structure of a complex $(n-1)$-dimensional manifold.

\begin{definition}
A \emph{complex (coordinate) chart} on a topological space $M$ is a homeomorphism $\phi\colon V\to O\in\mathbb{C}^n$ from some open set $V \subset M$ to an open set $O$ of $\mathbb{C}^n$ with the standard topology for some dimension $n$.
\end{definition}

It is convenient to denote a chart $\phi$ defined on $V\subset M$ with $(V,\phi)$ explicitly marking its domain.

\begin{definition}
Let there be two complex charts $(V,\phi)$ and $(W,\psi)$, $V,W \subset M$, and let their ranges have the same dimensionality $n$. The homeomorphism $\phi\circ\psi^{-1}$ between $\psi(V\cap W) \subset \mathbb{C}^n$ and $\phi(V\cap W) \subset \mathbb{C}^n$ is called a \emph{transition function} (between charts $\psi$ and $\phi$).
\end{definition}

\begin{definition}
A topological space $M^n$ is called a \emph{complex $n$-dimensional manifold} if 1) there is a set of charts $\{(U_i,\phi_i)\}_{i\in I}$ ($I$ is some index set) of $M^n$ such that $\bigcup_{i\in I} U_i = M^n$, 2) all the ranges of $\phi_i$ have the same dimensionality $n$ and 3) all the transition functions $\phi_i\circ\phi_j^{-1}$ are holomorphic functions between open subset of $\mathbb{C}^n$. The system $\{(U_i,\phi_i)\}_{i\in I}$ is called a \emph{holomorphic atlas} on $M^n$.
\end{definition}

An atlas allows to work with a manifold using usual methods of linear algebra and analysis in $\mathbb{C}^n$. The standard atlas of charts that gives $\mathbb{C}P^{n-1}$ a structure of an $(n-1)$-dimensional complex manifold is given by $n$ special charts. For each $i$, $1 \leq i \leq n$, define $U_i = \{[\mathbf{z}]:z_i \neq 0\}$, label coordinates of $\mathbb{C}^{n-1}$ by $\zeta_1$, \ldots, $\zeta_{i-1}$, $\zeta_{i+1}$, \ldots, $\zeta_n$ ($i$-th label is omitted) and set $\zeta_k = z_k/z_i$, which defines a map $\phi_i\colon U_i\to\mathbb{C}^{n-1}$. This defines a complex chart on $\mathbb{C}P^{n-1}$ with the whole of $\mathbb{C}^{n-1}$ as its image. It is easy to see that $\{U_i,\phi_i\}_{1\leq i\leq n}$ forms a holomorphic atlas of $\mathbb{C}P^{n-1}$. The transition functions from $\phi_i$ with its range coordinatized by $\zeta_k$ and $\phi_j$ with its range coordinatized by $\eta_k$ (with the same labeling omission rule for each chart as above) is given by $\eta_k = \zeta_k/\zeta_j$ for $k \neq i,j$ and $\eta_i = 1/\zeta_j$. They are simple rational maps and therefore are holomorphic.

In fact, the action of $\phi_i$ on $[\mathbf{z}]$ can be understood as setting $z_i = 1$, rescaling the rest of $z_k$ accordingly and taking the whole tuple $(z_1,\ldots,z_n)$ as coordinates of a point in the original space $\mathbb{C}^n$. The image of the chart $(U_i,\phi_i)$ becomes an $(n-1)$-dimensional affine subspace of the original space. In the following we will abuse notations by denoting the domain of the $i$-th chart, the chart itself, and the corresponding affine subspace with the same symbol $U_i$. We will refer to these special charts as \emph{affine charts}. Geometric relationships between $U_i$ and the action of the transition functions as projections between them is schematically shown in \cref{sketch} (other notations on this sketch will be introduced below).

Polynomials and their systems can be used to define subsets on linear complex spaces.

\begin{definition}
Let $\{P_k\}_k$ be a system of polynomials in $n$ variables $\{z_i\}_i$. The subset $V = \{\mathbf{z} \in \mathbb{C}^n: \forall\,k\,P_k(z_1,\dots,z_n) = 0\}$ is called an \emph{algebraic variety}.
\end{definition}

A polynomial $P$ such that $\forall \alpha \in \mathbb{C}$ $P(\alpha z_1,\dots,\alpha z_n) = \alpha^k P(z_1,\dots,z_n)$ is called a \emph{homogeneous polynomial}. It is obvious that if a point $\mathbf{z}$ is a root of a homogeneous polynomial, then every representative of its equivalence class $[\mathbf{z}]$ is a root, too. Thus homogeneous polynomials can be used to define subsets in projective space.

\begin{definition}
Let $\{P_k\}_k$ be a system of homogeneous polynomials in $n$ variables $\{z_i\}_i$. The subset $V = \{[\mathbf{z}] \in \mathbb{C}P^{n-1}: \forall\,k\,P_k(z_1,\dots,z_n) = 0\}$ is called a \emph{projective variety}.
\end{definition}

Note that any projective variety defines an algebraic variety in each affine chart $U_i$.

\section{Eigenvectors as fixed points}\label{fixed-points}

Consider an $n\times n$ complex matrix $M$. Its eigenvectors are elements of $\mathbf{z} \in\mathbb C^n$.
%whose standard coordinates we denote $(z_i)_{1\leq i\leq N}$.
% (That is, to avoid confusion we use bold symbols for complex vectors and light symbols for their coordinates and other complex numbers; uppercase symbols defferent from $N$ are reserved to matrices.) 
%Let $Mz$ denote a standard matrix product of a matrix and a column-vector.
\begin{lemma}
The eigenvectors of $M$ are in one-to-one correspondence with nonzero solutions of the systems $\{\mathcal E_{ij}\}_{i,j}$ of polynomial equations in coordinates of $\mathbf{z}$ for all $i$ and $j$, where
$$
\mathcal E_{ij}:(M\mathbf{z})_j z_i = (M\mathbf{z})_i z_j.
$$
\end{lemma}

\begin{proof}
The system $\{\mathcal E_{ij}\}_{i,j}$ states that the tensor $M\mathbf{z}\otimes \mathbf{z} - \mathbf{z}\otimes M\mathbf{z}$ vanishes. This statement is equivalent to the statement that $M\mathbf{z}$ and $\mathbf{z}$ are collinear.
\end{proof}

As already noted, the eigenvectors of matrix $M$ are naturally identified with elements of the complex projective space $\mathbb{C}P^{n-1}$. From this point of view, the system $\{\mathcal E_{ij}\}_{i,j}$ defines a projective variety.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{plots/sketch}
    \caption{
        A schematic sketch of the geometrical relations between the original space $\mathbb{C}^n$, the projective space $\mathbb{C}P^{n-1}$, its affine charts, and points in these different spaces that correspond to certain eigenvectors. The projective space $\mathbb{C}P^{n-1}$ (in red) can be associated with the unit sphere in $\mathbb{C}^n$, in which each circle carved out in it by a complex line passing through the origin (a special case of a real plane) is glued to form a single point (here exemplified by two abstract points $p$ and $q$, the circles are degenerated to pairs of points in real settings). The standard basis $\{\mathbf{e}_k\}$ of $\mathbb{C}^n$ (eigenvectors of $D$) defines affine charts $U_k$ of $\mathbb{C}P^{n-1}$ (only two are depicted: for $k=i$ in blue and $k=j$ in green). $U_i$ and $U_j$ are shown as affine subspaces of $\mathbb{C}^n$. Note that $\mathbf{e}_j$ is at infinity of $U_i$ and $\mathbf{e}_i$ is at infinity of $U_j$. An actual eigenvector and all its nonzero rescalings correspond to a single point in $\mathbb{C}P^{n-1}$ and in each $U_k$. Two eigenvectors $\mathbf{z}_i$ and $\mathbf{z}_j$ are depicted with a rescaling choice such that they are equal to the $i$-th and $j$-th columns of matrix $\Z$.
    }
    \label{sketch}
\end{figure}

Fix an index $i$. To find the full set of eigenvectors in a single chart $U_i$ it is enough to solve a smaller set of equations.

\begin{lemma}\label{subsystem-equivalence}
Eigenvectors $\mathbf z$ of $M$ such that $[\mathbf z] \in U_i$ are in one-to-one correspondence with the solutions of the system of equations $\{\mathcal E_{ij}\}_{j:j\neq i}$ in the same chart.
\end{lemma}

\begin{proof}
We need to prove that for $[\mathbf{z}] \in U_i$ the two following propositions are equivalent: $(i)$ $\mathbf{z}$ is a root of $(M\mathbf{z})_j z_i = (M\mathbf{z})_i z_j$ for $j \neq i$ and $(ii)$ $\exists \eigval \in \mathbb C$ $M\mathbf{z} = \eigval \mathbf{z}$.

The $(i)$ $\Rightarrow$ $(ii)$ direction. For $[\mathbf{z}] \in U_i$ it is enough to consider only vectors with $z_i = 1$. Then from $\{\mathcal E_{ij}\}_{j:j\neq i}$ we conclude that $(M\mathbf{z})_j = (M\mathbf{z})_i z_j$ for all $j \neq i$. If we now denote the common factor in these expressions as $\eigval = (M\mathbf{z})_i$, then $(ii)$ immediately follows.

The $(ii)$ $\Rightarrow$ $(i)$ direction. Consider an eigenvector $\mathbf{z}$ of $M$ such that $z_i \neq 0$, and thus $[\mathbf{z}] \in U_i$. From the eigenvalue
equation $M\mathbf{z}=\eigval \mathbf{z}$ we can express its eigenvalue as $\eigval=(M \mathbf{z})_i/z_i$; inserting this back into $M \mathbf{z}=\eigval \mathbf{z}$ shows that $\mathbf{z}$ is a root of the system $\{\mathcal E_{ij}\}_{j:i\neq j}$.
\end{proof}

Denoting $V_i$ the projective variety defined by the system $\{\mathcal E_{ij}\}_{j:j\neq i}$ for a fixed $i$ and recalling that the set $\{U_i\}_i$ forms an atlas we conclude that the set of eigenvectors of $M$ can be identified with the set $\cup_i(V_i\cap U_i)$, which is in fact the projective variety $\cap_i V_i$. Since eigenvectors generically have non-zero coordinates in all directions, each component $V_i\cap U_i$ typically contains the complete set of eigenvectors.

It should be noted, however, that $V_i$ taken alone may have points unrelated to the eigenvectors of $M$, but these additional points can only be at infinity in $U_i$ by Lemma~\ref{subsystem-equivalence}. Indeed, consider points of $V_i$ such that $z_i = 0$. At the very least they include points that are given by $(M\mathbf{z})_i = 0$ in addition to $z_i = 0$. Thus, in general, there is a whole ($n-3$)-dimensional complex projective subspace of such solutions in $\mathbb CP^{n-1}$ (corresponding to a ($n-2$)-dimensional complex hyperplane in the affine space $\mathbb C^n$).



We now further assume a partitioning
$$M=D+\Delta$$
with a diagonal part $D$ and the residual part $\Delta$. The motivation comes from physics, where the diagonal part $D$ often consists of unperturbed
eigenvalues $\evzero_i=D_{ii}$ and $\Delta$ represents perturbations in the so-called perturbation theory. In practice $D$ can be taken as the diagonal elements of $M$, although different partitionings can sometimes be more appropriate \cite{Surj_n_2004}. Provided the $\evzero_i$'s are all simple (non-degenerate, that is pairwise different), we can rewrite the polynomial system $\{\mathcal E_{ij}\}_{j:j\neq i}$ for a particular fixed $i$ as
$$
(D\mathbf{z} + \Delta\mathbf{z})_j z_i = (D\mathbf{z} + \Delta\mathbf{z})_i z_j\ \textrm{for}\ j\neq i,
$$
or equivalently
$$
\evzero_j z_j z_i + (\Delta\mathbf{z})_j z_i = \evzero_i z_i z_j + (\Delta\mathbf{z})_i z_j\ \textrm{for}\ j\neq i,
$$
which yields
$$
z_i z_j=(\evzero_j-\evzero_i)^{-1}\left(z_j(\Delta \mathbf{z})_i-z_i(\Delta \mathbf{z})_j\right)\ \textrm{for}\ j\neq i.
$$
In the affine subspace $U_i$ this gives (by setting $z_i = 1$)
$$
z_j=(\evzero_j-\evzero_i)^{-1}\left(z_j(\Delta \mathbf{z})_i-(\Delta \mathbf{z})_j\right)\ \textrm{for}\ j\neq i.
$$
Thus, the eigenvectors of $M$ in $U_i$ can be identified with the solutions of the fixed point equation $\mathbf z = \mathbf{f}_i(\mathbf z)$ with $\mathbf{f}_i:\mathbb C^n\to \mathbb C^n$ the map 
\begin{equation}
	\label{single_line_dyn} 
	\mathbf{f}_i(\mathbf{z}) = \mathbf{e}_i + \mathbf{g}_i\circ(\mathbf{z} (\Delta \mathbf{z})_i - \Delta \mathbf{z})
\end{equation}
where $\mathbf{e}_i$ is the $i$-th standard basis vector of $\mathbb{C}^n$, $\mathbf{g}_i$ is the $i$-th column of the \emph{inverse gaps matrix} $G$ with components $G_{jk} = (\mathbf g_k)_j = (\evzero_j-\evzero_k)^{-1}$ for $j\neq k$ and $G_{jj} = (\mathbf g_j)_j = 0$. Here $\circ$ denotes the component-wise product of vectors in the standard basis. To obtain the eigenvector of $M$ closest to the basis vector $\mathbf{e}_i$, we can try to solve \cref{single_line_dyn} by fixed-point iteration; this is the basic idea of our method, presented in Algorithm \ref{single_line_algo}. As noted, each iteration of $\mathbf f_i$ consists of a single multiplication of the present vector by the perturbation $\Delta$, followed by element-wise multiplication by the vector $\mathbf{g}_i$. The resulting solution will be automatically normalized such that its $i$-th coordinate is equal to 1.


\begin{algorithm}[t]
\caption{One eigenvector of a near-diagonal matrix $M$}\label{single_line_algo}
\begin{algorithmic}[1]
 
    \STATE{Choose a partition $M = D + \Delta$ with $D$ diagonal}
    \STATE{Compute $\mathbf{g}_i \leftarrow ((D_{jj}-D_{ii})^{-1})_{j:j\neq i}$ and set $(\mathbf g_i)_i =0$}
    \STATE Initialize $\mathbf{z} \leftarrow \mathbf{e}_i$
    \WHILE{$\vert\mathbf z - \mathbf{f}_i(\mathbf z)\vert/\vert\mathbf z\vert >\eta$ for some tolerance $\eta$}        \STATE{$\mathbf z \leftarrow \mathbf{f}_i(\mathbf z)$ with $\mathbf{f}_i$ defined by \cref{single_line_dyn}}
    \ENDWHILE
    \STATE{Return eigenvector $\mathbf{z}$}
    \STATE{Return eigenvalue $\eigval_i = D_{ii} + (\Delta \mathbf{z})_i$}

\end{algorithmic}
\end{algorithm}

The same iterative technique can be used to compute all eigenvectors of $M$ in parallel (Algorithm \ref{full_spectrum_algo}). For this it suffices to bundle all $n$ candidate
eigenvectors for each $i$ into a matrix $\Z$ and apply the map $\mathbf{f}_i$ to the $i$-th column of $\Z$. This corresponds to the matrix map

\begin{equation}
\label{dyn} 
F(\Z)\equiv I+G\circ\Big(\Z\,\mathcal{D}(\Delta \Z) - \Delta \Z\Big),
\end{equation}
where $\,\circ\,$ denotes the Hadamard (element-wise) product of matrices and $\mathcal{D}(X)$ is the diagonal matrix built with the diagonal elements of matrix $X$. Starting from $\Z^{(0)} = I$, we obtain a sequence of matrices $\Z^{(k)} = F(\Z^{(k-1)})$ whose limit as $k\to\infty$, if exists, is the full set of eigenvectors. We call this approach \textit{iterative perturbation theory} (IPT).

\begin{algorithm}[t]
\caption{Full eigendecomposition of a near-diagonal matrix $M$}
\begin{algorithmic}[1]
     \label{full_spectrum_algo}
    \STATE{Choose a partition $M = D + \Delta$ with $D$ diagonal}
    \STATE{Compute $G \leftarrow ((D_{ii}-D_{jj})^{-1})_{i\neq n}$ and set $g_{ii} =0$}
    \STATE{Initialize $\Z \leftarrow I$}
    \WHILE{$\Vert \Z -F(\Z)\Vert/\Vert \Z\Vert > \eta $ for some tolerance $\eta$}
        \STATE{$\Z \leftarrow F(\Z)$ with $F$ defined by \cref{dyn}}
    \ENDWHILE
    \STATE{Return eigenmatrix $\Z$}
    \STATE{Return eigenvalues $\Eigval = D + \mathcal{D}(\Delta \Z)$}
\end{algorithmic}
\end{algorithm}

\section{Convergence and divergence}\label{convergence}

In this section we look at the convergence of fixed-point iteration for the map \cref{dyn}. In a nutshell, the off-diagonal elements $\Delta$ must be small compared to the diagonal gaps $\evzero_i-\evzero_j$, a typical condition for eigenvector perturbation theory \cite{kato1995}. 

\subsection{A sufficient condition for convergence}

Let $\|\cdot\|$ denote the spectral norm of a matrix, i.e. its largest singular value. Let $M \in \mathbb C^{n\times n}$ be a matrix. Let $M = D + \Delta$ be its partition into a diagonal matrix $D$ and the residual matrix $\Delta$ such that the corresponding to $D$ matrix of inverse gaps $G$ is defined, which implies that all diagonal elements of $D$ are pairwise different. Let $F:\mathbb C^{n\times n} \to \mathbb C^{n\times n}$ be the mapping defined by $G$ and $\Delta$ as in the previous section.

\begin{theorem}\label{main-th}
If

\begin{equation}
\label{condition-unique}
\|G\|\|\Delta\| < 3 - 2\sqrt{2},
\end{equation}
then the dynamical system defined by iterations of $F$ and $A^{(0)} = I$ converges to a unique, asymptotically stable fixed point in the ball $B_{\sqrt{2}}(I)$.
\end{theorem}

\begin{proof}

The proof uses the Banach fixed-point theorem based on the $\|\cdot\|$ norm, which is sub-multiplicative with respect to both the matrix and Hadamard products~\cite{johnson2012}.

First, the estimate

\[\Vert F(\Z)-I\Vert\leq \Vert G\Vert \Vert\Delta\Vert (\Vert \Z\Vert+\Vert \Z\Vert^2)\]
implies that \(F\) maps a closed ball \(B_r(I)\) of radius \(r\) centered on \(I\) onto itself whenever \(\Vert G\Vert\Vert\Delta\Vert\leq r/[(1+r)(2+r)]\).  Next, from {\cref{dyn}} we have the estimate

\[
\Vert F(\Z)-F(\Z')\Vert\leq \Vert G\Vert \Vert\Delta\Vert\,(1+\Vert \Z+  \Z'\Vert)\, \Vert \Z -  \Z'\Vert.
\]
Hence, \(F\) is contracting
in \(B_r(I)\) provided \(\Vert G\Vert\Vert\Delta\Vert < 1/[1+2(1+r)]\). When both conditions on $\|G\|\|\Delta\|$ hold the Banach fixed-point theorem implies
that \(\Z^{(k)}=F^k(I)\) converges exponentially to a unique fixed point $\Z^*$
within $B_r(I)$ as \(k\to\infty\). Choosing the optimal
radius
$$\underset{r>0}{\textrm{argmax}}\min\,\left(\frac{r}{(1+r)(2+r)},\frac{1}{1+2(1+r)}\right)=\sqrt{2},$$
we see that \(\|G\|\Vert\Delta\Vert<3-2\sqrt{2}\approx 0.17\) guarantees convergence to the fixed point $\Z^*$.
\end{proof}

The set of solutions of the equation $\Z = F(\Z)$ contains matrices composed of all combinations of eigenvectors of $M$ with the appropriate normalization (the $i$-th coordinate of the column at the $i$-th position is set to 1). Some solutions may contain several repeated eigenvectors. Such solutions are rank-deficient, viz. $\operatorname{rank} \Z < n$. In principle, there is a danger that the iterative algorithms converges to one such solution with a loss of information about the eigenvectors of $M$ as a result.
The following theorem guarantees that under condition (\ref{condition-unique}) this does not happen.

\begin{theorem}
\label{theorem-rank}
Let condition (\ref{condition-unique}) hold for a partition of matrix $M$ and $\Z^*$ be the unique fixed point of $F$ in $B_{\sqrt{2}}(I)$. Then $\Z^*$ has full rank.
\end{theorem}

\begin{proof}
The proof uses additional lemmas and one theorem provided in \cref{appendix:full_rank}.

A square matrix is rank-deficient if either two of its columns are collinear or more than two of its columns are linearly dependent but not pairwise collinear. For $\Z^*$ the latter is only possible if the corresponding eigenvectors belong to an eigenspace of $M$ of dimension higher than one, which is ruled out by Lemma~\ref{lemma-eigenspace}. By Lemma~\ref{lemma-defective}, $\Z^*$ does not contain any defective eigenvectors of $M$. Then the rank can be lost only by a repetition of an eigenvector (with renormalization) corresponding to an eigenvalue of multiplicity one.

Embed $M$ into the family $M_\parameter = D + \parameter \Delta$ with $\parameter \in [0,1]$, so that $M_1 = M$ and $M_0 = D$. Consider a partition for each member of the family $M_\parameter = D_\parameter + \Delta_\parameter$ such that $D_\parameter = D$ and $\Delta_\parameter = \parameter\Delta$. Let $G_\parameter = G$ be the inverse gaps matrix of $D_\parameter$ and $F_\parameter$ be the mapping defined by $G_\parameter$ and $\Delta_\parameter$ according to (\ref{dyn}) with a substitution of $G$ by $G_\parameter$ and $\Delta$ by $\Delta_\parameter$. It follows that $\|G_\parameter\|\|\Delta_\parameter\| < 3 - 2\sqrt{2}$ holds for all $\parameter$. Thus, according to Theorem~\ref{main-th}, each $F_\parameter$ has a unique fixed point $\Z^*_\parameter$ in $B_{\sqrt{2}}(I)$. Furthermore, by Lemmas~\ref{lemma-eigenspace} and \ref{lemma-defective} we can be sure that none of the columns of $\Z^*_\parameter$ for each value of $\parameter$ corresponds either to an eigenvector with algebraic multiplicity higher than one or lays in an eigenspace of $M_\parameter$ with dimension higher than one. Thus they all correspond to eigenvalues that are simple roots of the characteristic polynomial of $M_\parameter$. Then all projective points that induce columns of $\Z^*_\parameter$ are differentiable functions of $\parameter$ by Theorem~\ref{theorem-continuous}. It means that if some two columns of $\Z^* = \Z^*_1$ are collinear (and thus correspond to the same projective point), the same columns stay collinear in $\Z^*_\parameter$ for all $\parameter$. But none of the columns of $I = \Z^*_0$ are collinear. We conclude that $\Z^*$ cannot have collinear columns.
\end{proof}

\begin{corollary}
If $M$ has eigenvalues with multiplicity higher than one, then for any partition $\tilde M = \tilde D + \tilde\Delta$ of any matrix $\tilde M$ similar to $M$, where $\tilde D$ is diagonal such that the corresponding inverse gaps matrix $\tilde G$ is defined, the relation $\|\tilde G\| \|\tilde\Delta\| \geq 3 - 2\sqrt{2}$ holds.
\end{corollary}

The following subsections are dedicated to aspects of IPT related to quantum-mechanical perturbation theory (sec. \ref{IPT-RS}) and dynamical systems theory (sec. \ref{2d-example}). They may be omitted by the non-specialist reader. 

\subsection{Contrast with RS perturbation theory}\label{IPT-RS}

It is interesting to constrast the present iterative method with conventional RS
perturbation theory, where the eigenvectors of a parametric matrix $M = D + \parameter \Delta$ are
sought as power series 
in $\parameter$, \emph{viz.} $\mathbf z=\sum_{\ell}\parameter^\ell \mathbf z^{[\ell]}$, where vector-terms (so called corrections) $\mathbf z^{[\ell]}$ do not depend on $\parameter$. Provided that $D$ has distinct diagonal entries, it is indeed possible to express the matrix of eigenvectors $\Z$ (with the same normalization as in Section~\ref{fixed-points} and with the order of eigenvectors such that $\Z \to I$ as $\parameter \to 0$) as a power series $\Z = \sum_\ell \parameter^\ell \Z^{[\ell]}$, which converges in some disk around $\parameter = 0$ \cite{kato1995}. Then the order-$k$ approximation of $\Z$ takes the form
$\Z^{(k)}_{\textrm{RS}}=\sum_{\ell=0}^k \parameter^\ell \Z^{[\ell]}$. The matrix
corrections $\Z^{[\ell]}$ are obtained from $\Z^{[0]}=I$
via the recursion
(\cref{appendix:RSPT})

\begin{equation}\label{ARS-alpha} 
\Z^{[\ell]}=G\circ\left(\sum_{s=0}^{\ell-1} \Z^{[\ell-1-s]}\,\mathcal{D}(\Delta \Z^{[s]}) -\Delta \Z^{[\ell-1]}\right).
\end{equation}
The iterative
scheme $ \Z^{(k)}$ completely contains this RS series in the
sense that $\Z^{(k)}=\Z^{(k)}_{\textrm{RS}}+\mathcal{O}(\parameter^{k+1})$; this can be seen by induction (\cref{appendix:IPT_RSPT}). In other words, we can recover the usual perturbative expansion
of $\Z$ to order $k$ by
iterating $k$ times the map $F$ and
dropping all terms $\mathcal{O}(\parameter^{k+1})$. Moreover, the parameter whose
smallness determines the convergence of the RS series is the product of
the perturbation magnitude $|\parameter|$ with the inverse diagonal
gaps $\|G\|$ \cite{kato1995}, just as it determines the
contraction property of $F$.

But IPT also differs from
the RS series in two key ways. First, the complexity of each iteration
is constant (essentially just one matrix product with $\Delta$), whereas
computing the RS corrections $\Z^{[\ell]}$ involves the sum of
increasingly many matrix products. Second, not being defined as a power
series, the convergence of $ \Z^{(k)}$ when $k\to\infty$
is not \emph{a priori } restricted to a disk in the
complex $\parameter$-plane. Together, these two differences
suggest that IPT has the potential to converge faster,
and in a larger domain, than RS perturbation theory. This is what we now
examine, starting from an elementary but explicit example.

\begin{figure*}
\includegraphics[width=\textwidth]{plots/fig1}
\caption{Convergence of IPT in the two-dimensional
example \cref{2by2}. Left: In the
complex $\parameter$-plane, RS perturbation theory (RS-PT)
converges inside a circle of radius $1/2$ (orange line)
bounded by the exceptional points $\pm i/2$ where eigenvalues
have branch-point singularities and $M$ is not
diagonalizable. Dynamical perturbation theory (IPT) converges inside
the domain bounded by the blue cardioid, which is larger---especially along the
real axis, where there is no singularity. Outside this domain, the map
can converge to a periodic cycle, be chaotic or diverge to infinity, following flip bifurcations (along the real axis)
and fold bifurcations (at the singularities of the  blue curve). The domain where the map
remains bounded (black area) is a conformal transformation of the
Mandelbrot set. Right: The bifurcation diagram for the quadratic
map $f$ along the real $\parameter$-axis
illustrates the period-doubling route to chaos as $\parameter$
increases away from $0$ (in absolute value). Orange and
left vertical lines indicate the boundary of the convergence domains of
RS-PT and IPT respectively. }
\label{2d}
\end{figure*}

\subsection{An explicit $2\times 2$ example}\label{2d-example}

To build intuition, let us consider the parametric matrix 
\begin{equation}
\label{2by2} 
M =
\begin{pmatrix}
   0 & \parameter \\
   \parameter & 1  
\end{pmatrix}
.
\end{equation}
This matrix has eigenvalues $\eigval_{\pm}=(1\pm\sqrt{1+4\parameter^2})/2$, both of which are analytic inside the disk $\vert\parameter\vert<1/2$ but have branch-point singularities at $\parameter=\pm i/2$. (These singularities are exceptional points, \textit{i.e.} $M$ is not diagonalizable for these values.) Because the RS series is a power series, these imaginary points contaminate its convergence also on the real axis, where no singularity exists: $\Z_{\textrm{RS}}$ diverges for any value of $\parameter$ outside the disk of radius $1/2$, and in particular for real $\parameter>1/2$ .

Considering instead our iterative scheme, one easily computes $$\Z^{(k)}=
\begin{pmatrix}
    1 & -f^k(0) \\
    f^k(0) & 1
    \end{pmatrix}
,$$
where $f(x)=\parameter(x^2-1)$ and the superscripts indicate $k$-fold iterates. This one-dimensional map has two fixed points at $x^*_{\pm}=\eigval_{\pm}/\parameter$. Of these two fixed points $x^*_+$ is always unstable, while $x^*_-$ is stable for $\parameter \in(-\sqrt{3}/2, \sqrt{3}/2)$ and loses its stability at $\parameter=\pm\sqrt{3}/2$ in a flip bifurcation. At yet larger values of $\parameter$, the iterated map $f^k$---hence the fixed-point iterations $ \Z^{(k)}$---follows the period-doubling route to chaos familiar from the logistic map \cite{May_1976}. For values of $\parameter$ along the imaginary axis, we find that the map is stable if $\Im \parameter\in(-1/2,1/2)$ and loses stability in a fold bifurcation at the exceptional points $\parameter=\pm i/2$. The full domain of convergence of the system is strictly larger than the RS disk, as shown in \cref{2d}. We also observe that the disk where both schemes converge, the dynamical scheme does so with a better rate than RS perturbation theory: we check that $\vert f^k(0)-x^*_-\vert\sim\vert1-\sqrt{1+4\parameter^2}\vert^k=\mathcal{O}(\vert 2\parameter^2\vert^k)$, while the remainder of the RS decays as $\mathcal{O}(\vert 2\parameter\vert^k)$. This is a third way in which the dynamical scheme outperforms the usual RS series, at least in this case: not only is each iteration computationally cheaper, but the number of iterations required to reach a given precision is lower.

Although it is possible to analytically compute the convergence domain of our iterative scheme in this simple case, in practice this task is prohibitive even for very small matrices, as it can be seen on a slightly more complicated $3\times 3$ example. This example along with the computation of the convergence domain for the $2\times 2$ case can be found in \cref{further-examples}

\section{Performance}\label{performance}

\begin{figure}
\begin{center}

\includegraphics[width=.8\textwidth]{plots/fig5}

		
\end{center}
	\caption{Runtime vs. dimension $N$ for dense, non-symmetric perturbative matrices of the form \cref{near-diagonal} in single precision. Blue lines refer to IPT and orange lines to LAPACK's GEEV routine, implemented on 128 CPU cores (continuous lines) or on a Titan V GPU (dashed lines). Within its perturbative convergence domain, IPT is up to $32$ and $91$ times faster than GEEV, respectively.}
	\label{scaling}
\end{figure}


We now compare the efficiency and accuracy of IPT to reference eigenvalues routines. Our system consists of a dual AMD EPYC 7702 with $128$ CPU cores using using LAPACK, ARPACK and PRIMME XXX and a NVidia V100 GPU using Nvidia cuSOLVER XXX. Test matrices are of the form
\begin{equation}\label{near-diagonal}
M = \textrm{diag}(n)_{1\leq n \leq N} + \parameter R
\end{equation}
where $R$ is an array of standard normal random variables and $\parameter$ a positive parameter. We consider four cases: $R$ can either be dense or sparse (with density $50/N$, corresponding to $\sim 50N$ non-zero elements), and non-symmetric or symmetric (via $(R+R^T)/2$). For each case the time required to compute all $N$ eigenpairs in double precision is denoted $T_{\textrm{eig}}$, and the time to compute the lowest-lying eigenpair is denote $T_{\textrm{eigs}}$. As a reference we use the time for one matrix-matrix multiplication $T_{\textrm{mm}}$ or one matrix-vector multiplication $T_{\textrm{mv}}$, respectively. In all figures below we use continuous lines for experiments on the CPU and dashed lines for experiments on the GPU, and brackets indicate the wrapper we used to call the relevant eigenvalue routine. Unless specified otherwise, the error tolerance is $\eta = 100\epsilon$, where $\epsilon \simeq 2.2 \cdot 10^{-16}$ (resp. $\epsilon \simeq 1.2 \cdot 10^{-7}$) is machine epsilon in double (resp. single) precision. 



\subsection{Full spectrum}


\begin{figure*}
\includegraphics[width=\textwidth]{plots/fig3}
\caption{IPT vs. LAPACK for the complete diagonalization of perturbative matrices with increasing $\parameter$, in double precision. The largest speed-ups are obtained for the non-symmetric problem.}
\label{full_cpu}	
\end{figure*}


For the complete diagonalization problem we compare IPT to MATLAB's \textbf{eig} function, which calls LAPACK's routines GEEV for non-symmetric matrices and SYEV for symmetric matrices; on the GPU MAGMA is used instead. 

 \cref{full_cpu} shows the corresponding CPU timings in the dense, sparse, symmetric and non-symmetric cases ($N=4096$). We make two observations. First, unlike LAPACK, our algorithm has a similar complexity with symmetric and non-symmetric matrices, leading to larger speed-ups in the latter case. Second, because it is based on matrix-matrix multiplication, IPT performs especially well on sparse matrices. This is another difference with standard full spectrum algorithms, which do not take advantage of sparsity. 

As noted earlier, Hessenberg reduction is inefficient for matrices which are already near diagonal. In the next experiment we compare IPT with Nvidia's GPU implementation of Jacobi's algorithm, which does not rely on reduction and is known to have local quadratic convergence. In spite of this advantage, Jacobi (labeled SYEVJ in cuSOLVER) only proved faster than divide-and-conquer SYEVD for small matrices sizes; for $N=8182$ SYEVJ is slower than SYEVD for all values of $\parameter$ (\cref{full_gpu}, center and right panels). By contrast, our implementation of IPT in CUDA is faster than SYEVD for $\parameter \leq 0.05$. For dense, non-symmetric matrices, IPT is much faster than GEEV for all $\parameter$ compatible with convergence (\cref{full_gpu}, top-left panel).   

\begin{figure*}
\includegraphics[width=\textwidth]{plots/fig2}
\caption{Timing of full spectrum routines on the GPU for dense perturbative matrices. Here IPT proves to be faster than both Divide and Conquer (SYEVD) and Jacobi (SYEVJ) when $\parameter$ is sufficiently small.}
\label{full_gpu}	
\end{figure*}


\subsection{One eigenpair}

Next we consider the computation of just the lowest eigenpair of the matrices above, corresponding to the column $n=1$. Here performance comparisons are based on ARPACK (Lanczos/Arnoldi) and PRIMME (PReconditioned Iterative Multi-Method Eigensolver), which were the most efficient existing implementations of iterative methods we tested. PRIMME includes standard Krylov methods as well as preconditioned eigensolvers including Generalized Davidson (GD) \cite{Davidson_1975, morgan1986generalizations}, Jacobi-Davidson (JD) \cite{sleijpen2000jacobi} and, for symmetric problems, LOBPCG \cite{knyazev2017recent}; if desired, PRIMME can shift dynamically between GD and JD, leading to the smallest number of matrix-vector iterations possible. When using preconditioned eigensolvers, we use a diagonal (`Jacobi') preconditioner that takes advantage of the near-diagonal structure of $M$. For a discussion of Davidson-type methods in comparison with Lanczos, see \cite{morgan1986generalizations}. Finally, we note that shift-and-invert methods were tested on the near-diagonal problems \cref{near-diagonal}, but proved less efficient than all methods reported here. 

 \cref{one} presents our results. In all cases (dense, sparse, symmetric or non-symmetric) IPT runs much faster than ARPACK. Davidson, an algorithm designed for diagonally dominant matrices and used e.g. in quantum chemistry applications, provides a more meaningful reference point. Here we find that IPT is up to $\sim 5$x faster at small $\parameter$. This can be explained by the fact that, although IPT requires a similar number of iterations as Davidson, each iteration is cheaper, with just one matrix-vector multiplication per step. 



\begin{figure*}
\includegraphics[width=\textwidth]{plots/fig4}
\caption{IPT vs. standard iterative algorithms for the lowest-lying eigenpair of perturbative matrices of the form \cref{near-diagonal} and increasing $\parameter$. Davidson-type methods with diagonal preconditioning outperform the Krylov-Schur algorithm on these problems, and IPT is yet faster when the perturbation is sufficiently small (or sparse).}
\label{one}	
\end{figure*}

\subsection{Parallelism}

Underlying the performance of IPT is the high parallelism of matrix-vector (BLAS 2) and, especially, matrix-matrix (BLAS 3) multiplication. Such parallelism is not shared by Householder reflections, which is why eigenvalue routines based on Hessenberg or tri-diagonal reduction do not scale as well on multi-core or GPU architectures. We illustrate this in \cref{parallel}, where the time to diagonalize a dense, non-symmetric matrix with $\parameter = 0.1$ is plotted as a function of the number of active CPU cores. As expected from its simple structure, IPT parallelizes just as well as matrix-matrix multiplication (almost perfectly with the number of cores). On large clusters with thousands of cores, we expect IPT will outperform GEEV by several orders of magnitude.


\begin{figure*}
\begin{center}
\includegraphics[width=0.6\textwidth]{plots/fig6}
\end{center}
\caption{Parallel scaling. Unlike direct methods based on Hessenberg reduction (here GEEV for a near-diagonal matrix of the form \cref{near-diagonal} with parameters as indicated), IPT scales as well as BLAS (here matrix-matrix multiplication). As a result, the speed-up of IPT over GEEV is only limited by the number of CPU cores available.}
\label{parallel}	
\end{figure*}




\subsection{Accuracy}

Finally we compared the accuracy of IPT on the full spectrum problem with LAPACK via Julia's \textbf{eigen} function. For this we used near-diagonal matrices of the form \cref{near-diagonal} with $N=1024$ in double precision varying $\parameter\in[10^{-4}, 0.2]$. For each $\parameter$ we ran IPT and \textbf{eig} to obtain the matrix of eigenvectors $\Z$ and corresponding eigenvalues $\Eigval=\textrm{diag}(\eigval_n)_{1\leq n\leq N}$. We then measured the accuracy of each routine through the residual error $\Vert M\Z-\Z \Eigval\Vert_F$, where $\Vert\cdot\Vert_F$ denotes the Frobenius norm. We find that IPT is more than an order of magnitude more accurate than LAPACK, with a median residual error of $4.4\cdot 10^{-11}$ and $6.4\cdot 10^{-10}$ respectively.

\section{Applications}\label{applications} In this final section we consider two possible applications of IPT: as a mixed-precision eigenvalue algorithm (full spectrum problem), and as a potential competitor to Davidson in quantum chemistry (lowest-lying eigenvector). 

\subsection{A mixed-precision eigenvalue algorithm}

The condition that a matrix be near-diagonal of course severely restricts the applicability of IPT, although relevant examples exist in quantum chemistry, see below. However, IPT can be used more generally as a \textit{refinement} method for well-conditioned eigenvalue problems. Consider a generic matrix $M$ with well-separated eigenvalues, and assume given an approximation of its eigenmatrix $\Z_0$. For instance, we could compute $\Z_0$ using standard drivers in single precision, resulting in a single precision eigenmatrix $\Z_{0, s}$. Next, we convert this matrix to double precision, denoted $\Z_{0, d}$, and apply IPT to the near-diagonal matrix $M' = \Z_{0, d}^{-1}M \Z_{0, d}$. Once iterations have converged to a new eigenmatrix $\Z'$, we can rotate back and obtain accurate eigenvectors for $M$ as $\Z = \Z_{0,d} \Z'$. In practice the linear solve and matrix multiplication steps are fast, and so computing $\Z$ takes about the same time as the low-precision diagonalization $\Z_{0,s}$. 

%When applied to general random matrices (with independent normally distributed entries), this mixed-precision approach gives speed-ups over DGEEV (double-precision GEEV) of $\sim2-3$x at the cost of $\sim 10$x larger residual errors. (We did not attempt to lower these errors through balancing or else.) 



Obviously, this method cannot be used for ill-conditioned eigenvalue problems, because then $(i)$ diagonal gaps become large and IPT ceases to converge, and $(ii)$ the condition number of $\Z_{0, d}$ becomes large and accuracy is lost in the linear solve step. To measure the robustness of the mixed-precision algorithm to eigenvalue clustering, we considered matrices of the form 
\begin{equation}\label{ill_cond}
J_{\alpha} = Q^{T}D_\alpha Q\quad\textrm{with}\ D_\alpha=\textrm{diag}(10^{-\alpha n/N})_{1\leq n\leq N},
\end{equation}
where $\alpha$ is a parameter controlling the spacing between eigenvalues and $Q$ a random orthogonal matrix. For matrices of size $N=1024$ we found that IPT remained applicable up to $\alpha \simeq 4$, corresponding to a minimal spectral gap of $1.5\cdot10^{-6}$. (For this matrix \textbf{eigs} failed to converge with the default parameters.) For all values of $\alpha$ below this threshold, we obtain acceptable residual errors ($\sim 5$x larger than DGEEV) and significant speed-ups over DGEEV ($2-3$x faster), see  \cref{refinement}.



\begin{algorithm}[t]
\caption{Iterative refinement of eigenvectors of matrix $M$ with well-separated eigenvalues}
\begin{algorithmic}[1]   
    \STATE Compute eigenvectors $\Z_{0,s}$ in single precision \textit{e.g.} using SGEEV or SSYEV
    \STATE Set $\Z_{0,d}$ as $\Z_{0,s}$ in double precision
    \STATE Compute $M'\leftarrow \Z_{0,d}^{-1} M \Z_{0,d}$ using a linear solver in double precision
    \STATE Compute eigenvectors $\Z'$ and $\Eigval$ using IPT($M'$)
    \STATE Return eigenvectors $\Z = \Z_{0,d}\Z'$
    \STATE Return eigenvalues $\Eigval$
\end{algorithmic}    
\end{algorithm}

\subsection{Full configuration interaction}\label{FCI}

Finally, we tested IPT on a real-world problem from quantum chemistry, the full configuration interaction (FCI) approach to \textit{ab initio} electronic structure calculation \cite{ostlund1996}. Given a molecule, its geometry (the position of nuclei) and a basis set, FCI provides the most accurate estimate of the energy and wavefunction of electrons available. Formally, an approximation of the eigenvectors of the Hamiltonian matrix is first computed with self-consistent-field (Hartree-Fock) methods; in that basis the Hamiltonian matrix $H$ is near-diagonal (and symmetric), and FCI proceeds by computing iteratively the eigenvector $z_0$ with smallest eigenvalue $\eigval_0$, usually using the Davidson algorithm. Because the dimension $N$ of the Hamiltonian matrix $H$ for $n_e$ electrons distributed in $n_b$ basis sets is $(n_b\ \textrm{choose}\ n_e)$ (hence grows factorially with system size), the bottleneck in  FCI computations is the matrix-vector product $H\mathbf z$. (Fig. \ref{H2O_matrix} shows the sparsity pattern of a small FCI matrix.) 


 In order to reduce the number of iterations as much as possible, we applied fixed-point acceleration to the iteration $\mathbf z \leftarrow \mathbf f_0(\mathbf z)$ in Algorithm \ref{single_line_algo}. Several acceleration methods are available, e.g. Anderson acceleration based on the QR factorization of a tall matrix at each iteration \cite{Walker_2011} or the recent Alternating Cyclic Extrapolation (ACX) XXX, which uses a linear combination of $2$ or $3$ applications of $f_0$ per iteration. 
 

We compared IPT with ACX acceleration (IPT-ACX) with the implementation of the Davidson algorithm provided in the open-source quantum chemistry suite PySCF \cite{PYSCF} (using the function \textbf{pyscf.lib.davidson}); for a python implementation of Anderson acceleration we used the code from Ref. \cite{zhang2020globally}. The convergence criterion was $\Vert H\mathbf z_0 - \eigval_0 \mathbf z_0\Vert \leq 10^{-8}$. 
 
 
 \begin{figure*}
\centering
\includegraphics[scale = 0.4]{plots/H20_sto-3g.pdf}
\caption{Sparsity pattern of a small FCI matrix, here for $\mathrm{H}_2\mathrm{O}$ in the minimal basis set sto-3g.}
\label{H2O_matrix}
\end{figure*}
\begin{center}

 \begin{tabular}[t]{|c c c c c|} 
 \hline
 Molecule & Basis set & $N$ & ($K$, $T$) Davidson &  ($K$, $T$) IPT-AA \\
 \hline
 He & cc-pvqz & 900 & (9, 0.098) & (8, 0.055)  \\ 
 \hline
 Be & cc-pvtz & 189225 & (13, 1.82) & (13, 1.82) \\
  \hline
$\mathrm{H}_2\mathrm{O}$ & sto-6g & 441 & (9, 0.003) & (8, 0.002) \\
 \hline
$\mathrm{H}_2\mathrm{O}$ & 6-31G & 1656369 & (18, 5.36) & (20, 4.75) \\
 \hline
 LiH & 6-31g & 3025 & (12, 0.028) & (12, 0.027) \\
  \hline
 LiH & cc-pvtz & 894916 & (15, 39.95) & (15, 41.86) \\
  \hline
\end{tabular}\label{fci_table}
\captionof{table}{Full configuration interaction calculation on small molecules, using different geometries and basis sets. With Anderson acceleration (memory $m=5$), IPT is equally efficient as the Davidson algorithm which is standard in the field. Here $K$ and $T$ denote the number of matrix-vector evaluations and total CPU time (in s) respectively.}
\end{center}


As showed in Table 1, we find that IPT-ACX converges for realistic FCI problems with a number of iterations and timing competitive with Davidson. One conceptual difference between the two methods, however, is that IPT is self-contained, whereas Davidson relies on other eigenvalue algorithms to obtain Ritz values. 


\section{Discussion}

We have presented a new eigenvalue algorithm for near-diagonal matrices, be them symmetric or non-symmetric, dense or sparse. IPT can be applied to obtain a single perturbed eigenvector, with performance comparable to state-of-the-art preconditioned eigensolvers. IPT can also be applied to the full spectrum problem; in that case IPT benefits from a lower theoretical complexity and lower runtime than standard methods based on Hessenberg reduction. The largest speed-ups are obtained for non-symmetric, full spectrum problems. 

We emphasize that the near-diagonality condition is quite restrictive: off-diagonal matrix elements must be small compared to diagonal spacings. While their are important applications with this property (e.g. full configuration interaction), it is fair to say that IPT is more akin to a refinement procedure than a full-fledged eigenvalue algorithm. Even so IPT can be useful: using a mixed-precision approach we computed the eigenvectors of a general dense matrix with random entries to double precision in a fraction of the time required by DGEEV. 

Future work should focus on stabilizing our procedure for larger perturbations. For instance, when the perturbation is too large and IPT blows up, we may shrink $\parameter$ it to $\parameter/Q$ for some integer $Q$, diagonalize the matrix with this smaller
perturbation, restart the algorithm using the diagonal matrix thus
obtained as initial condition, and repeat the
operation $Q$ times. This approach is similar to the
homotopy continuation method for finding polynomial
roots \cite{Morgan_2009} and can effectively extend the domain of
convergence of the present iterative scheme. Another idea is to leverage the
projective-geometric structure outlined above, for
instance by using charts on the complex projective space other
than $z_i=1$, which would lead to different maps with
different convergence properties. A third possibility is to use the
freedom in choosing the diagonal matrix $D$ to construct maps
with larger convergence domains, a trick which is known to sometimes
improve the convergence of the RS series \cite{Surj_n_2004}. Finally, we saw that the convergence of IPT can be improved using Anderson acceleration, which can be viewed as an \textit{ad hoc} non-linear Krylov subspace method. It would be interesting to see whether there exists an optimal combination of previous iterates to find the fixed points of the IPT map.


\begin{figure*}
\includegraphics[width=\textwidth]{plots/fig7}
\caption{IPT as a refinement algorithm for ill-conditioned problems \cref{ill_cond} with increasingly small eigengap. Combining single-precision GEEV with IPT gives eigenvectors with acceptable accuracy at a fraction of the computational cost of double-precision GEEV.}
\label{refinement}	
\end{figure*}

\section*{Acknowledgments}

We thank Rostislav Matveev and Alexander Heaton for useful discussions.


\bibliographystyle{ieeetr}
\bibliography{biblio.bib}


\newpage
\appendix

\section{Additional lemmas for Section~\ref{convergence}}\label{appendix:full_rank}

Here as in the main text we consider an $n \times n$ matrix $M$ with complex entries along with its partition $M = D + \Delta$, where $D$ is diagonal with pairwise distinct diagonal elements. The partition defines a mapping $F\colon \mathbb C^{n\times n} \to \mathbb C^{n\times n}$ as in the main text.

\begin{lemma}
\label{lemma-eigenspace}
Let $M = D + \Delta$ be a matrix with a partition that obeys (\ref{condition-unique}). Then there are no eigenvectors of $M$ from eigenspaces of dimension higher than one as columns of the corresponding unique fixed point $\Z^*$ of $F$ in $B_{\sqrt{2}}(I)$.
\end{lemma}

\begin{proof}
If $\Z^*$ contains an eigenvector from an eigenspace of $M$ with dimension higher than one, then any matrix $\Z$ obtained by a substitution in $\Z^*$ of that eigenvector with a vector from this eigenspace is a fixed point of $F$, too. This means that there is a whole subspace of fixed points of $F$ of dimension higher than zero which includes $\Z^*$. But this contradicts the uniqueness of $\Z^*$ in $B_{\sqrt{2}}(I)$.
\end{proof}

\begin{lemma}
\label{lemma-defective}
If $M$ and its partition obey (\ref{condition-unique}) and $M$ is defective, then the unique fixed point $\Z^*$ of $F$ in $B_{\sqrt{2}}$ does not contain any eigenvector causing the defect.
\end{lemma}

\begin{proof}
By Lemma~(\ref{lemma-eigenspace}), it is enough to assume that $M$ does not have geometrically multiple eigenvalues and all its eigenvectors are isolated projective points, so all fixed points of $F$ are isolated matrices. Let such $M$ be defective. Let $J$ be a Jordan normal form of $M$ and $S$ be the transition matrix that turns $M$ into $J$. Consider a deformation $J_{\mathbf d}$ of $J$ given by $J_{\mathbf d} = J + \diag(\mathbf d)$, where $\mathbf d \in \mathbb C^n$. This induces a deformation of $M$ given by $M_{\mathbf d} = M + S\diag(\mathbf d) S^{-1}$. It is clear that one can find $\mathbf d$ arbitrarily close to 0 (in the standard topology of $\mathbb C^n$) such that all diagonal entries of $J_{\mathbf d}$ are different. As $J_{\mathbf d}$ is upper triangular, its eigenvalues (and thus those of $M_{\mathbf d}$) are equal to its diagonal entries. But then $M_{\mathbf d}$ has a full eigenbasis of isolated eigenvectors. The formerly merged eigenvectors split into distinct ones. Furthermore, the new eigenvectors that split from an old defective eigenvector can be made arbitrarily close to it (as projective points in the standard topology of $\mathbb CP^{n-1}$) by choosing $\mathbf d$ close enough to 0. This is obvious from considering the eigenvectors spawned from a single block of $J_{\mathbf d}$ and their behavior at the limit $\mathbf d \to 0$.

All this implies that there is a deformation $M_{\mathbf d}$ of $M$ such that in its partition $M_{\mathbf d} = D + \Delta_{\mathbf d}$, $\Delta_{\mathbf d} = \Delta + S\diag(\mathbf d)S^{-1}$, $\|\Delta_{\mathbf d} - \Delta\|$ is arbitrarily small and thus $\|G\| \|\Delta_{\mathbf d}\| < 3 - 2\sqrt{2}$ still holds. At the same time, $\Z^*$ splits into several arbitrarily close to it, and thus still contained in $B_{\sqrt{2}}(I)$ for small enough $\mathbf d$, fixed points of the mapping $F_{\mathbf d}$ defined by the partition, with two or more of them being full-rank and multiple associated lower rank fixed points. But this contradicts the uniqueness of the fixed point of $F_{\mathbf d}$ computed for the deformed matrix $M_{\mathbf d}$.
\end{proof}

\begin{theorem}[\cite{Lax2007} Theorem 8, page 130]
\label{theorem-continuous}
Let $M_\parameter$ be a differentiable matrix-valued function of real $\parameter$, $a_\parameter$ an eigenvalue of $M_\parameter$ of multiplicity one. Then we can choose an eigenvector $\mathbf h_\parameter$ of $M_\parameter$ pertaining to the eigenvalue $a_\parameter$ to depend differentiably on $\parameter$.
\end{theorem}

The theorem guarantees that the eigenvector corresponding to an eigenvalue with multiplicity one is a differentiable function of the parameter as a projective space valued function.

\section{Rayleigh-Schr\"odinger perturbation theory}\label{appendix:RSPT}

Here we recall the derivation of the Rayleigh-Schr\"odinger recursion, given \emph{e.g.} in \cite{Roth_2010}. The idea behind the original perturbation theory is the following. Let a parametric family of matrices $M_\parameter = D + \parameter \Delta$ be given, where $\parameter \in \mathbb C$ is the parameter, $M_\parameter \in \mathbb C^{n\times n}$, $M_0 = D$ is diagonal and is called the \emph{unperturbed} matrix, and $\Delta$ is the \emph{perturbation} matrix. The problem is to find eigenvectors of $M_\parameter$ in the form of an asymptotic series in powers of the parameter $\parameter$. We specifically assume a generic case of the unperturbed matrix $D$ having distinct diagonal entries. The perturbation matrix $\Delta$ can be arbitrary.

Let $\eigval_i \in \mathbb C$ and $\mathbf z_i \in \mathbb C^n$ respectively be the $i$-th eigenvalue and the $i$-th eigenvector of $M_\parameter$ ordered such that
\begin{equation}\label{ordering}
\lim_{\parameter \to 0}\eigval_i = \evzero_i, \quad \lim_{\parameter\to 0}\mathbf z_i=\mathbf e_i,
\end{equation}
where $\evzero_i = D_{ii}$, $\mathbf e_i$ are the standard basis vectors of $\mathbb C^n$, and normalized according to $\langle \mathbf e_i,\mathbf z_i\rangle =1$ for all $\parameter$ (a choice known in physics as ``intermediate normalization''), where by $\langle\cdot,\cdot\rangle$ we understand the standard scalar product in $\mathbb C^n$. Such normalization corresponds to singling out eigenvectorst from the affine charts $U_i$ seen as affine subspaces of $\mathbb C^n$ as defined in Section~\ref{fixed-points} of the main text. Therefore, the matrix composed of $\mathbf z_i$ in the chosen order is matrix $\Z$ from the main text. The possibility to order the eigenvectors in such a way that (\ref{ordering}) holds follows from the well known fact that, given the condition on $D$ and the chosen normalization, $\eigval_i$ and $\mathbf z_i$ are holomorphic in $\parameter$ in a certain disk around 0, $\mathbf z_i$ are distinct, and these functions are given by series convergent in that disk \cite{kato1995}:
\begin{equation}\label{power-series}
\eigval_i=\sum_{\ell\geq 0}\parameter^{\ell}\eigval_{i}^{[\ell]}\quad\textrm{and}\quad \mathbf z_i=\sum_{\ell\geq 0}\parameter^{\ell}\mathbf z_{i}^{[\ell]}.
\end{equation}
As a consequence, matrix $\Z$ itself is holomorphic in the same disk and can be represented by a power series in $\parameter$
\begin{equation}\label{A-series}
\Z = \sum_{\ell \geq 0} \parameter^\ell \Z^{[\ell]}.
\end{equation}

Substituting expressions (\ref{power-series}) into the eigenvalue equation $M\mathbf z=\eigval \mathbf z$ and making use of the Cauchy product formula, this yields $D\mathbf z^{[0]}_i=\eigval^{[0]}_i\mathbf z^{[0]}_i$ at zeroth order (hence $\eigval^{[0]}_i=\evzero_i$) and for $\ell\geq 1$
$$(D-\evzero_i I)\mathbf z_{i}^{[\ell]}=\sum_{s=1}^{\ell}\eigval_{i}^{[s]}\mathbf z_{i}^{[\ell-s]}-\Delta \mathbf z_{i}^{[\ell-1]},$$
where $I$ is the unit $n\times n$ matrix.

It is convenient to expand $\mathbf z_{i}^{[\ell]}$ in the basis of the eigenvectors of $D$ as
$\mathbf z_{i}^{[\ell]}=\sum_{j=1}^{n}\Z_{ji}^{[\ell]} \mathbf e_j$ with $\Z_{ij}^{[0]}=\delta_{ij}$ and $\Z_{ii}^{[\ell]}=0$ for $\ell\geq 1$. By construction, $\Z_{ij}^{[\ell]}$ are elements of matrices $\Z^{[\ell]}$ from (\ref{A-series}). This gives for each $\ell\geq 1$ and $1\leq j\leq n$
$$
(\evzero_j-\evzero_i)\Z_{ji}^{[\ell]}=\sum_{s=1}^{\ell}\eigval_{i}^{[s]}\Z_{ji}^{[\ell-s]}-(\Delta \Z^{[\ell-1]})_{ji}.
$$
The equation for the eigenvalues correction is extracted by swapping $i$ with $j$ in this equation and using $\Z_{ii}^{[\ell]}=\delta_{\ell,0}$. This leads to
$\eigval_{i}^{[\ell]}=(\Delta \Z^{[\ell-1]})_{ii}.$
Injecting this back into the equation above we arrive at
\begin{equation}\label{RS-recursion}
\Z_{ij}^{[\ell]}=G_{ij}\Big(\sum_{s=1}^{\ell} (\Delta \Z^{[s-1]})_{jj} \Z_{ij}^{[\ell - s]} - (\Delta \Z^{[\ell-1]})_{ij}\Big).
\end{equation}

\section{Iterative perturbation theory contains the RS
series}\label{appendix:IPT_RSPT}

We prove $\Z^{(k)}  = \Z_\mathrm{RS}^{(k)} + \mathcal O(\parameter^{k+1})$ by induction. Obviously $\Z^{(0)} = \Z_\mathrm{RS}^{(0)} = I$. Suppose that $\Z^{(k-1)} = \Z_\mathrm{RS}^{(k-1)} + \mathcal O(\parameter^k)$ or, more specifically,
$$
\Z^{(k-1)} = \sum_{\ell = 0}^{k-1} \parameter^\ell  \Z^{[\ell]} + \mathcal O(\parameter^k),
$$
where the matrices $\Z^{[\ell]}$ satisfy the recursion (\ref{RS-recursion}). Then from $\Z^{(k)} = F(\Z^{(k-1)})$ we have
$$
\Z^{(k)} = I +\parameter G\circ\left(
\sum_{m=0}^{k-1} \parameter^m \Z^{[m]} \mathcal D \left(\sum_{\ell=0}^{k-1} \parameter^\ell \Delta \Z^{[\ell]} \right) -
\sum_{\ell=0}^{k-1}\parameter^{\ell} \Delta \Z^{[\ell]} \right) + \mathcal O(\parameter^{k+1}).
$$
From this expression it is easy to see that the term of $s$-th order in $\parameter$  in $\Z^{(k)}$ is given by terms with $\ell + m = s - 1$, \emph{viz.} 
$$
\parameter^s G \circ \left(
\left(\sum_{\ell=0}^{s-1} \Z^{[s-1-\ell]} \mathcal D \left(\Delta \Z^{[\ell]}\right)\right)
- \Delta \Z^{[s-1]}
\right).
$$
This term matches exactly the RS correction term $\parameter^s \Z   ^{[s]}$. This concludes the proof.

\section{Explicit examples}\label{appendix:examples}
\label{further-examples}

\subsection{A general approach to find the convergence domain}

Following the notations of the main text, consider the map $\mathbf f_i$ for some fixed $i$. An attracting equilibrium point of the corresponding dynamical system $\mathbf z^{(k)}= \mathbf f_i(\mathbf z^{(k-1)})$ loses its stability when the Jacobian matrix $\partial \mathbf f_i$ of $\mathbf f_i$, $(\partial \mathbf f_i)_{js} \equiv \partial (\mathbf f_i)_j/\partial z_s  = \partial F_{ji}/\partial z_s$, has an eigenvalue (called \emph{multiplier} in this case of a discrete-time dynamical system) with absolute value equal to $1$ at this point. The convergence domain of the dynamical system \cref{dyn} for the whole matrix $\Z$ of the eigenvectors is equal to the intersection of the convergence domains for its individual lines \cref{single_line_dyn}.

Consider the system of $n+1$ polynomial equations of $n+2$ complex variables ($z_j$ for $1\leq j\leq n$, $\parameter$, and $\mu$)

$$
\begin{cases}
\mathbf z = \mathbf f_i(\mathbf z),\\
\det(\partial \mathbf f_i - \mu I) = 0.
\end{cases}
\label{system-one} 
$$
The variable $\mu$ here plays the role of a multiplier of a steady state. Either by successively computing resultants or by constructing a Groebner basis with the correct lexicographical order, one can exclude the variables $z_j$ from this system, which results in a single polynomial of two variables $(\parameter,\mu) \mapsto P(\parameter,\mu)$. This polynomial defines a complex 1-dimensional variety. The projection to the $\parameter$-plane of the real 1-dimensional variety defined by $\{P=0, \vert\mu\vert^2= 1\}$ corresponds to some curve $C$. A more informative way is to represent this curve as a complex function of a real variable $t$ implicitly defined by $P(\parameter,e^{it}) = 0$.

The curve $C$ is the locus where a fixed point of $\mathbf f_i$ have a multiplier on the unit circle. In particular, the fixed point that at $\parameter = 0$ corresponds to $z_i = 1$ and $z_j = 0$, $j \neq i$, loses its stability along a particular subset of this curve. The convergence domain of the iterative perturbation theory is the domain that is bounded by these parts of the curve and that contains $0$.

\subsection{The $2\times 2$ example of the main text}

According to the procedure outlined above, the set where IPT loses stability ($C$, the blue curve in  \cref{2d}) is found to be given by the parametric equation $4\parameter^2 + e^{it}(2 - e^{it}) = 0$ (same equation for both eigenvectors). The cusps (return points, the points where $d\parameter/dt = 0$) of this curve are at $\parameter = \pm i/2$. In this particular case, the convergence circle of the RS perturbation theory is completely contained in the convergence domain of the iterative perturbation theory, and their boundaries intersect only at the cusp points. This convergence domain for IPT is directly related to the main cardioid of the classical Mandelbrot set: the set of complex values of the parameter $c$ that lead to a bound trajectory of the classical quadratic (holomorphic) dynamical system $x^{(k+1)} = (x^{(k)})^2 + c$. The main cardioid of the Mandelbort set (the domain of stability of a steady state) is bounded by the curve $4c - e^{it}(2 - e^{it}) = 0$. The boundary of the stability domain of our $2\times2$ example is simply a conformal transform of this cardioid by two complementary branches of the square root function composed with the sign inversion. The origin of this relation becomes obvious after the parameter change $c \mapsto -\parameter^2$ followed by the variable change $x \mapsto \parameter x$. This brings the classical system to the dynamical system of the only nontrivial component of the first line for our $2\times 2$ example: $x^{(k+1)} = \parameter \left((x^{(k)})^2 - 1\right)$. The nontrivial component of the second line follows an equivalent (up to the sign change of the variable) equation: $x^{(k+1)} = \parameter \left(1 - (x^{(k)})^2 \right)$.

This particular $2 \times 2$ example is very special. In fact, any 2-dimensional case is special in the following sense. The iterative approximating sequence for $M = D + \parameter \Delta$ for any $D$ and $\Delta$ takes the form
$$\Z^{(k)} =
\begin{pmatrix}
    1 & f_1^k(0) \\
    f_2^k(0) & 1
\end{pmatrix}
,$$
where $f_j$ are univariate quadratic polynomial functions related by $g_1(x) = x^2 g_2(1/x)$, with $g_j(x) \equiv x - f_j(x)$. The first special feature of this recursion scheme is that it is equivalent to a 1-dimensional quadratic discrete-time dynamical system for each column of $A$. This implies that the only critical point of either $f_j$ ($0$, when the diagonal elements of $\Delta$ are equal) is necessarily attracted by at most unique stable fixed point. The second special feature is fact that both columns have exactly the same convergence domains in the $\parameter$-plane. To see this, suppose that $x_1$ and $x_2$ are the roots of $g_1$. Then it follows that $1/x_1$ and $1/x_2$ are the roots of $g_2$. As $g_2'(1/x) = 2g_1(x)/x - g_1'(x)$, and (like for any quadratic polynomial) $g_1'(x_1) = -g_1'(x_2)$, we also see that $g_1'(x_{1,2}) = g_2'(1/x_{2,1})$, and thus $f_1'(x_{1,2}) = f_2'(1/x_{2,1})$. Therefore, the fixed point of the dynamical systems defined by $x^{(k+1)} = f_1(x^{(k)})$ corresponding to an eigenvector and the fixed point of $x^{(k+1)} = f_2(x^{(k)})$ corresponding to the different eigenvector are stable or unstable simultaneously.

These two properties are not generic when $n>2$. Therefore, we provide another explicit example of a $3 \times 3$ matrix to foster some intuition for more general cases.

\subsection{An additional $3\times 3$ example}

Consider the following parametric $3\times3$ matrix and its partition:
$$
M =
\begin{pmatrix}
0 & 0 & 0 \\
0 & 1 & 0 \\
0 & 0 & 3
\end{pmatrix}
+
\parameter
\begin{pmatrix}
0 & 1 & 2 \\
1 & 0 & 3 \\
2 & 3 & 0
\end{pmatrix}
.
$$
The polynomial $P$ that defines the fixed point degeneration curve $C$ here takes the form for $\mathbf z_1$

\begin{align}
P(\parameter,\mu) &= 63792 \parameter^{7} - 28352 \parameter^{6} \mu - 68040 \parameter^{6} - 29556 \parameter^{5} \mu^{2} + 89352 \parameter^{5} \mu\nonumber\\
&- 13239 \parameter^{5} + 960 \parameter^{4} \mu^{3}
 + 14516 \parameter^{4} \mu^{2} - 39164 \parameter^{4} \mu + 12116 \parameter^{4}\nonumber\\ 
&+ 5616 \parameter^{3} \mu^{4} - 26658 \parameter^{3} \mu^{3} + 29988 \parameter^{3} \mu^{2} - 546 \parameter^{3} \mu - 2448 \parameter^{3}\nonumber\\ 
&+ 468 \parameter^{2} \mu^{5} - 3720 \parameter^{2} \mu^{4} + 12820 \parameter^{2} \mu^{3} - 17648 \parameter^{2} \mu^{2} + 7584 \parameter^{2} \mu\nonumber\\ 
&- 1296 \parameter^{2} - 243 \parameter \mu^{6} + 1404 \parameter \mu^{5} - 2619 \parameter \mu^{4} + 1350 \parameter \mu^{3} + 432 \parameter \mu^{2}\nonumber\\ 
&+ 108 \mu^{6} - 792 \mu^{5} + 1980 \mu^{4} - 1872 \mu^{3} + 432 \mu^{2},
\label{poly-n1} 
\end{align}
for $\mathbf z_2$

\begin{align}
P(\parameter,\mu) &= 113408 \parameter^{7} - 63792 \parameter^{6} \mu - 120960 \parameter^{6} + 7416 \parameter^{5} \mu^{2} + 53208 \parameter^{5} \mu\nonumber\\  
&+ 36424 \parameter^{5} + 6525 \parameter^{4} \mu^{
3} - 11034 \parameter^{4} \mu^{2} - 13824 \parameter^{4} \mu - 5664 \parameter^{4}\nonumber\\ 
&- 3156 \parameter^{3} \mu^{4} - 2472 \parameter^{3} \mu^{3} + 10332 \parameter^{3} \mu^{2} + 3696 \parameter^{3} \mu + 3088 \parameter^{3}\nonumber\\  &- 72 \parameter^{2} \mu^{5} + 1800 \parameter^{2} \mu^{4} - 1800 \parameter^{2} \mu^{3} - 2088 \parameter^{2} \mu^{2} - 1296 \parameter
^{2} \mu\nonumber\\ 
&- 576 \parameter^{2} + 128 \parameter \mu^{6} - 24 \parameter \mu^{5} - 736 \parameter \mu^{4} - 120 \parameter \mu^{3} + 1328 \parameter \mu^{2}\nonumber\\ 
&- 72 \mu^{6} + 108 \mu^{5} + 360 \mu^{4} - 432 \mu^{3} - 288 \mu^{2},
\label{poly-n2} 
\end{align}
and for $\mathbf z_3$

\begin{align}
P(\parameter,\mu) &= 35440 \parameter^{7} - 42528 \parameter^{6} \mu - 37800 \parameter^{6} - 32360 \parameter^{5} \mu^{2} + 110080 \parameter^{5} \mu\nonumber\\ 
&- 23295 \parameter^{5} + 29112 \parameter^{4} \mu^
{3} - 4800 \parameter^{4} \mu^{2} - 88614 \parameter^{4} \mu + 51024 \parameter^{4}\nonumber\\ 
&+ 14640 \parameter^{3} \mu^{4} - 78760 \parameter^{3} \mu^{3} + 116760 \parameter^{3} \mu^{2} - 52920 \parameter^{3} \mu + 5400 \parameter^{3}\nonumber\\ 
&- 2376 \parameter^{2} \mu^{5} - 10152 \parameter^{2} \mu^{4} + 70296 \parameter^{2} \mu^{3} - 101496 \parameter^{2} \mu^{2} + 41904 \parameter^{2} \mu\nonumber\\ 
&- 864 \parameter^{2} - 1080 \parameter \mu^{6} + 7920 \parameter \mu^{5} - 19620 \parameter \mu^{4} + 19440 \parameter \mu^{3} - 6480 \parameter \mu^{2}\nonumber\nonumber\\ 
&+ 1296 \mu^{6} - 7992 \mu^{5} + 17712 \mu^{4} - 16416 \mu^{3} + 5184 \mu^{2}.
\label{poly-n3} 
\end{align}
As we can see, the dynamical systems for all three columns of $\Z$ ($\mathbf z_1$, $\mathbf z_2$, and $\mathbf z_3$) have different domains of convergence in the $\parameter$ plane. The corresponding curves are depicted in \cref{473965}, \cref{988969}, and \cref{824432}, respectively.

There are differences also in curves for individuals columns with those for the $2\times 2$ case. Note that a curve $C$ does not contain enough information to find the convergence domain itself. The domains on \cref{473965}--\cref{824432} were found empirically. Of course, they are bound by some parts of $C$ and include the point $\parameter = 0$. The reason for some parts of $C$ not forming the boundary of the domain is that its different parts correspond to different eigenvectors. In other words, they belong to different branches of a multivalued eigenvector function of $\parameter$, the cusps being the branching points.

Consider as a particular example the case $\mathbf z_2$. The curve $C$ intersects itself at $\parameter \approx -0.49$. Above the real axis about this point, one of the two intersecting branches of the curve form the convergence boundary. Below the real axis, the other one takes its place. This indicates that the two branches correspond to different multipliers of the same fixed point. When $\Im \parameter > 0$, one of them crosses the unitary circle at the boundary of the convergence domain;  when $\Im \parameter < 0$, the other one does. At $\parameter \approx -0.49$, both of them cross the unitary circle simultaneously. This situation corresponds, thus, to a Neimark-Sacker bifurcation (the discrete time analog of the Andronov-Hopf bifurcation). Both branches are in fact parts of the same continuous curve that passes through the point $\parameter \approx 0.56$. Around this point, the curve poses no problem to the convergence of the dynamical system. The reason for this is that an excursion around cusps (branching points of eigenvectors) permutes some eigenvectors. As a consequence, the curve at $\parameter \approx -0.49$ corresponds to the loss of stability of the eigenvector that is a continuation of the unique stable eigenvector at $\parameter = 0$ by the path $[0,-0.49]$. At the same time, the same curve at $\parameter \approx 0.56$ indicates a unitary by absolute value multiplier of an eigenvector that is not a continuation of the initial one by the path $[0,0.56]$.

Not all features of the curves depicted this $3\times3$ case are generic either. The particular symmetry with respect to the complex conjugation of the curve and of its cusps is not generic for general complex matrices $D$ and $\Delta$, but it is a generic feature of matrices with real components. In this particular case, due to this symmetry, the only possible bifurcations for real values of $\parameter$ are the flip bifurcations (a multiplier equals to $-1$, typically followed by the cycle doubling cascade), the Neimark-Sacker bifurcation (two multipliers assume complex conjugate values $e^{\pm it}$ for some $t$), and, if the matrices are not symmetric, the fold bifurcation (a multiplier is equal to $1$). With symmetric real matrices, the fold bifurcation is not encountered because the cusps cannot be real but instead form complex conjugated pairs. These features are the consequence of the behavior of $\det(D + \parameter \Delta - x I)$ with respect to complex conjugation and from the fact that symmetric real matrices cannot have nontrivial Jordan forms.

Likewise, Hermitian matrices result in complex conjugate nonreal cusp pairs, but the curve itself is not necessarily symmetric. As a result, there are many more ways for a steady state to lose its stability, from which the fold bifurcation is, however, excluded. Generic bifurcations at $\parameter \in \mathbb R$ here consist in a multiplier getting a value $e^{it}$ for some $t \neq 0$. The situation for general complex matrices lacks any symmetry at all. Here steady states lose their stability by a multiplier crossing the unit circle with any value of $t$, and thus the fold bifurcation, although possible, is not generic. It is generic for one-parameter (in addition to $\parameter$) families of matrices.

As already noted, for the holomorphic dynamics of  any $2\times2$ case the unique critical point is guaranteed to be attracted by the unique attracting periodic orbit, if the latter exists. This, in turn, guarantees that for any $\Delta$ with zero diagonal the iteration of $F$ starting from the identity matrix converges to the needed solution provided that $\parameter$ is in the convergence domain. This is not true anymore for $n > 2$, starting already from the fact that there are no discrete critical points in larger dimensions. The problem of finding a good initial condition becomes non-trivial. As can be seen on ~\cref{988969}, the particular $3\times3$ case encounters this problem for the second column ($\mathbf z_2$). The naive iteration with $\Z^{(0)} = I$ does not converge to the existing attracting fixed point of the dynamical system near some boundaries of its convergence domain. Our current understanding of this phenomenon is the crossing of the initial point by the attraction basin boundary (in the $\mathbf z$-space). This boundary is generally fractal. Perhaps this explains the eroded appearance of the empirical convergence domain of the autonomous iteration.

To somewhat mitigate this complication, we applied a nonautonomous iteration scheme in the form, omitting details, $\mathbf z^{(k+1)} = \mathbf f_2(\mathbf z^{(k)},\parameter (1 - \alpha^k))$ with $\mathbf z^{(0)} = (0,1,0)^T$, where $\alpha$ is some positive number $\alpha < 1$, so that $\lim_{k \to \infty} \parameter (1 - \alpha^k) = \parameter$, and we explicitly indicated the dependence of $\mathbf f_2(\mathbf z,\parameter)$ on $\parameter$. The idea of this \emph{ad hoc} approach is the continuation of the steady state in the extended $(\mathbf z,\parameter)$-phase space from values of $\parameter$ that put $\mathbf z^{(0)}$ inside the convergence domain of that steady state. Doing so, we managed to empirically recover the theoretical convergence domain (see ~\cref{988969}).

Finally, we would like to point out an interesting generic occurrence of a unitary multiplier without the fold bifurcation. For $\mathbf z_1$, this situation takes place at $\parameter \approx 0.45$, for $\mathbf z_2$ at $\parameter \approx 0.56$, and for $\mathbf z_3$ at $\parameter \approx 1.2$. All three points are on the real axis, as  is expected from the symmetry considerations above. There is no cusp at these points and no fold bifurcations (no merging of eigenvectors), as it should be for symmetric real matrices. Instead, another multiplier of the same fixed point goes to infinity at the same value of $\parameter$ (the point becomes super-repelling). As a result, the theorem of the reduction to the central manifold is not applicable.
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{plots/figSIn1}
\caption{{The convergence domain on the~\(\parameter\)-plane for the first
column of~\(\Z\)~(the first eigenvector \(\mathbf z_1\))
for the~\(3\times3\) example. The Mandelbrot-like set (domain where orbits remain bounded) of the iterative scheme is shown in black and grey. The empirical convergence domain is shown in black. Its largest component corresponds to the stability of a steady state (the applicability domain of the iterative method). Small components correspond to stability of various periodic orbits. Various shades of grey show the values of $\parameter$ that lead to divergence to infinity (the darker the slower the divergence). In red are the values of $\parameter$ where the matrix is non-diagonalizable.
{\label{473965}}%%
}}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{plots/figSIn2}
\caption{{{Same as ~{\cref{473965}} for the second
eigenvector~\(\mathbf z_2\) (the second column of \(\Z\)).
Small components correspond to stability of various periodic orbits.
Various shades of grey show the values of~\(\parameter\) that lead
to divergence to infinity (the darker the slower the divergence).
{\label{988969}}%
}%
}}
\end{center}
\end{figure}
\begin{figure}[H]
\begin{center}
\includegraphics[width=0.70\columnwidth]{plots/figSIn3}
\caption{{{Same as ~{\cref{473965}} for the third
eigenvector~\(\mathbf z_3\) (the third column of~\(\Z\)) .
{\label{824432}}%
}%
}}
\end{center}
\end{figure}


\end{document}