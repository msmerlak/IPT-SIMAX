\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{rayleigh1894}
\citation{kato1995}
\citation{rayleigh1894}
\citation{pozar1998}
\citation{ostlund1996}
\citation{demmel1997}
\citation{Davidson_1975}
\citation{Morgan_2009}
\citation{fokkema1998jacobi}
\citation{sleijpen2000jacobi}
\citation{knyazev2017recent}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}\protected@file@percent }
\newlabel{definitions}{{2}{2}{Definitions and notations}{section.3}{}}
\newlabel{definitions@cref}{{[section][2][]2}{[1][2][]2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Definitions and notations}{2}{section.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  A schematic sketch of the geometrical relations between the original space $\mathbb  {C}^n$, the projective space $\mathbb  {C}P^{n-1}$, its affine charts, and points in these different spaces that correspond to certain eigenvectors. The projective space $\mathbb  {C}P^{n-1}$ (in red) can be associated with the unit sphere in $\mathbb  {C}^n$, in which each circle carved out in it by a complex line passing through the origin (a special case of a real plane) is glued to form a single point (here exemplified by two abstract points $p$ and $q$, the circles are degenerated to pairs of points in real settings). The standard basis $\{\mathbf  {e}_k\}$ of $\mathbb  {C}^n$ (eigenvectors of $D$) defines affine charts $U_k$ of $\mathbb  {C}P^{n-1}$ (only two are depicted: for $k=i$ in blue and $k=j$ in green). $U_i$ and $U_j$ are shown as affine subspaces of $\mathbb  {C}^n$. Note that $\mathbf  {e}_j$ is at infinity of $U_i$ and $\mathbf  {e}_i$ is at infinity of $U_j$. An actual eigenvector and all its nonzero rescalings correspond to a single point in $\mathbb  {C}P^{n-1}$ and in each $U_k$. Two eigenvectors $\mathbf  {z}_i$ and $\mathbf  {z}_j$ are depicted with a rescaling choice such that they are equal to the $i$-th and $j$-th columns of matrix $Z$. \relax }}{3}{figure.caption.4}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{sketch}{{1}{3}{A schematic sketch of the geometrical relations between the original space $\mathbb {C}^n$, the projective space $\mathbb {C}P^{n-1}$, its affine charts, and points in these different spaces that correspond to certain eigenvectors. The projective space $\mathbb {C}P^{n-1}$ (in red) can be associated with the unit sphere in $\mathbb {C}^n$, in which each circle carved out in it by a complex line passing through the origin (a special case of a real plane) is glued to form a single point (here exemplified by two abstract points $p$ and $q$, the circles are degenerated to pairs of points in real settings). The standard basis $\{\mathbf {e}_k\}$ of $\mathbb {C}^n$ (eigenvectors of $D$) defines affine charts $U_k$ of $\mathbb {C}P^{n-1}$ (only two are depicted: for $k=i$ in blue and $k=j$ in green). $U_i$ and $U_j$ are shown as affine subspaces of $\mathbb {C}^n$. Note that $\mathbf {e}_j$ is at infinity of $U_i$ and $\mathbf {e}_i$ is at infinity of $U_j$. An actual eigenvector and all its nonzero rescalings correspond to a single point in $\mathbb {C}P^{n-1}$ and in each $U_k$. Two eigenvectors $\mathbf {z}_i$ and $\mathbf {z}_j$ are depicted with a rescaling choice such that they are equal to the $i$-th and $j$-th columns of matrix $\Z $. \relax }{figure.caption.4}{}}
\newlabel{sketch@cref}{{[figure][1][]1}{[1][3][]3}}
\@writefile{thm}{\contentsline {definition}{{Definition}{2.1}{}}{4}{theorem.5}\protected@file@percent }
\@writefile{thm}{\contentsline {definition}{{Definition}{2.2}{}}{4}{theorem.6}\protected@file@percent }
\@writefile{thm}{\contentsline {definition}{{Definition}{2.3}{}}{4}{theorem.7}\protected@file@percent }
\@writefile{thm}{\contentsline {definition}{{Definition}{2.4}{}}{4}{theorem.8}\protected@file@percent }
\@writefile{thm}{\contentsline {definition}{{Definition}{2.5}{}}{4}{theorem.9}\protected@file@percent }
\@writefile{thm}{\contentsline {definition}{{Definition}{2.6}{}}{5}{theorem.10}\protected@file@percent }
\@writefile{thm}{\contentsline {definition}{{Definition}{2.7}{}}{5}{theorem.11}\protected@file@percent }
\newlabel{fixed-points}{{3}{5}{Eigenvectors as fixed points}{section.12}{}}
\newlabel{fixed-points@cref}{{[section][3][]3}{[1][5][]5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Eigenvectors as fixed points}{5}{section.12}\protected@file@percent }
\@writefile{thm}{\contentsline {lemma}{{Lemma}{3.1}{}}{5}{theorem.13}\protected@file@percent }
\@writefile{thm}{\contentsline {proof}{{Proof}{1}{}}{5}{proof.14}\protected@file@percent }
\@writefile{thm}{\contentsline {lemma}{{Lemma}{3.2}{}}{5}{theorem.15}\protected@file@percent }
\newlabel{subsystem-equivalence}{{3.2}{5}{Eigenvectors as fixed points}{theorem.15}{}}
\newlabel{subsystem-equivalence@cref}{{[lemma][2][3]3.2}{[1][5][]5}}
\@writefile{thm}{\contentsline {proof}{{Proof}{2}{}}{5}{proof.16}\protected@file@percent }
\citation{Surj_n_2004}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.1}{\ignorespaces One eigenvector of a near-diagonal matrix $M$\relax }}{6}{algorithm.17}\protected@file@percent }
\newlabel{single_line_algo}{{3.1}{6}{One eigenvector of a near-diagonal matrix $M$\relax }{algorithm.17}{}}
\newlabel{single_line_algo@cref}{{[algorithm][1][3]3.1}{[1][6][]6}}
\newlabel{single_line_dyn}{{3.1}{6}{Eigenvectors as fixed points}{equation.26}{}}
\newlabel{single_line_dyn@cref}{{[equation][1][3]3.1}{[1][6][]6}}
\citation{kato1995}
\citation{johnson2012}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3.2}{\ignorespaces Full eigendecomposition of a near-diagonal matrix $M$\relax }}{7}{algorithm.28}\protected@file@percent }
\newlabel{full_spectrum_algo}{{3.2}{7}{Full eigendecomposition of a near-diagonal matrix $M$\relax }{algorithm.28}{}}
\newlabel{full_spectrum_algo@cref}{{[algorithm][2][3]3.2}{[1][7][]7}}
\newlabel{dyn}{{3.2}{7}{Eigenvectors as fixed points}{equation.27}{}}
\newlabel{dyn@cref}{{[equation][2][3]3.2}{[1][7][]7}}
\newlabel{convergence}{{4}{7}{Convergence and divergence}{section.37}{}}
\newlabel{convergence@cref}{{[section][4][]4}{[1][7][]7}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Convergence and divergence}{7}{section.37}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}A sufficient condition for convergence}{7}{subsection.38}\protected@file@percent }
\@writefile{thm}{\contentsline {theorem}{{Theorem}{4.1}{}}{7}{theorem.39}\protected@file@percent }
\newlabel{main-th}{{4.1}{7}{A sufficient condition for convergence}{theorem.39}{}}
\newlabel{main-th@cref}{{[theorem][1][4]4.1}{[1][7][]7}}
\newlabel{condition-unique}{{4.1}{7}{A sufficient condition for convergence}{equation.40}{}}
\newlabel{condition-unique@cref}{{[equation][1][4]4.1}{[1][7][]7}}
\@writefile{thm}{\contentsline {proof}{{Proof}{3}{}}{7}{proof.41}\protected@file@percent }
\@writefile{thm}{\contentsline {theorem}{{Theorem}{4.2}{}}{8}{theorem.42}\protected@file@percent }
\newlabel{theorem-rank}{{4.2}{8}{A sufficient condition for convergence}{theorem.42}{}}
\newlabel{theorem-rank@cref}{{[theorem][2][4]4.2}{[1][8][]8}}
\@writefile{thm}{\contentsline {proof}{{Proof}{4}{}}{8}{proof.43}\protected@file@percent }
\@writefile{thm}{\contentsline {corollary}{{Corollary}{4.3}{}}{8}{theorem.44}\protected@file@percent }
\citation{kato1995}
\citation{kato1995}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Convergence of IPT in the two-dimensional example \cref  {2by2}. Left: In the complex $\varepsilon $-plane, RS perturbation theory (RS-PT) converges inside a circle of radius $1/2$ (orange line) bounded by the exceptional points $\pm i/2$ where eigenvalues have branch-point singularities and $M$ is not diagonalizable. Dynamical perturbation theory (IPT) converges inside the domain bounded by the blue cardioid, which is larger---especially along the real axis, where there is no singularity. Outside this domain, the map can converge to a periodic cycle, be chaotic or diverge to infinity, following flip bifurcations (along the real axis) and fold bifurcations (at the singularities of the blue curve). The domain where the map remains bounded (black area) is a conformal transformation of the Mandelbrot set. Right: The bifurcation diagram for the quadratic map $f$ along the real $\varepsilon $-axis illustrates the period-doubling route to chaos as $\varepsilon $ increases away from $0$ (in absolute value). Orange and left vertical lines indicate the boundary of the convergence domains of RS-PT and IPT respectively. \relax }}{9}{figure.caption.45}\protected@file@percent }
\newlabel{2d}{{2}{9}{Convergence of IPT in the two-dimensional example \cref {2by2}. Left: In the complex $\parameter $-plane, RS perturbation theory (RS-PT) converges inside a circle of radius $1/2$ (orange line) bounded by the exceptional points $\pm i/2$ where eigenvalues have branch-point singularities and $M$ is not diagonalizable. Dynamical perturbation theory (IPT) converges inside the domain bounded by the blue cardioid, which is larger---especially along the real axis, where there is no singularity. Outside this domain, the map can converge to a periodic cycle, be chaotic or diverge to infinity, following flip bifurcations (along the real axis) and fold bifurcations (at the singularities of the blue curve). The domain where the map remains bounded (black area) is a conformal transformation of the Mandelbrot set. Right: The bifurcation diagram for the quadratic map $f$ along the real $\parameter $-axis illustrates the period-doubling route to chaos as $\parameter $ increases away from $0$ (in absolute value). Orange and left vertical lines indicate the boundary of the convergence domains of RS-PT and IPT respectively. \relax }{figure.caption.45}{}}
\newlabel{2d@cref}{{[figure][2][]2}{[1][9][]9}}
\newlabel{IPT-RS}{{4.2}{9}{Contrast with Rayleigh-Schr\"odinger perturbation theory}{subsection.46}{}}
\newlabel{IPT-RS@cref}{{[subsection][2][4]4.2}{[1][9][]9}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Contrast with Rayleigh-Schr\"odinger perturbation theory}{9}{subsection.46}\protected@file@percent }
\newlabel{ARS-alpha}{{4.2}{9}{Contrast with Rayleigh-Schr\"odinger perturbation theory}{equation.47}{}}
\newlabel{ARS-alpha@cref}{{[equation][2][4]4.2}{[1][9][]9}}
\citation{May_1976}
\newlabel{2d-example}{{4.3}{10}{An explicit $2\times 2$ example}{subsection.48}{}}
\newlabel{2d-example@cref}{{[subsection][3][4]4.3}{[1][10][]10}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}An explicit $2\times 2$ example}{10}{subsection.48}\protected@file@percent }
\newlabel{2by2}{{4.3}{10}{An explicit $2\times 2$ example}{equation.49}{}}
\newlabel{2by2@cref}{{[equation][3][4]4.3}{[1][10][]10}}
\citation{eyert1996comparative}
\citation{Walker_2011}
\citation{lepage2021alternating}
\citation{lepage2021alternating}
\citation{lepage2021alternating}
\citation{stathopoulos2010primme}
\citation{hernandez2005slepc}
\@writefile{loa}{\contentsline {algorithm}{\numberline {5.1}{\ignorespaces Alternating Cyclic Extrapolation \cite  {lepage2021alternating}, with vector of orders $o = (3, 2, 3, 2, \cdots  )$\relax }}{11}{algorithm.51}\protected@file@percent }
\newlabel{ACX}{{5.1}{11}{Alternating Cyclic Extrapolation \cite {lepage2021alternating}, with vector of orders $o = (3, 2, 3, 2, \cdots )$\relax }{algorithm.51}{}}
\newlabel{ACX@cref}{{[algorithm][1][5]5.1}{[1][11][]11}}
\newlabel{acceleration}{{5}{11}{Acceleration}{section.50}{}}
\newlabel{acceleration@cref}{{[section][5][]5}{[1][11][]11}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Acceleration}{11}{section.50}\protected@file@percent }
\newlabel{performance}{{6}{11}{Performance}{section.67}{}}
\newlabel{performance@cref}{{[section][6][]6}{[1][11][]11}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Performance}{11}{section.67}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Few eigenvalues}{11}{subsection.68}\protected@file@percent }
\citation{Julia-2017}
\citation{IPT-code}
\citation{paper-code}
\citation{ostlund1996}
\citation{Davidson_1975}
\citation{PYSCF}
\citation{morgan2000preconditioning}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces  IPT vs. PRIMME preconditioned eigensolvers for the FCI computation of the ground state of the molecular Hamiltonian of $\textrm  {H}_2$O ($n = 441$) in the minimal basis set ``sto-3g". Left: convergence history, i.e. norm of the residual vector $\mathbf  r = \mathbf  {f}(\mathbf  {z}) - \mathbf  {z}$ vs. matvecs, showing best performance for IPT and IPT-ACX. Right: the corresponding timings on our multi-core CPU. \relax }}{12}{figure.caption.70}\protected@file@percent }
\newlabel{water}{{3}{12}{IPT vs. PRIMME preconditioned eigensolvers for the FCI computation of the ground state of the molecular Hamiltonian of $\textrm {H}_2$O ($n = 441$) in the minimal basis set ``sto-3g". Left: convergence history, i.e. norm of the residual vector $\mathbf r = \mathbf {f}(\mathbf {z}) - \mathbf {z}$ vs. matvecs, showing best performance for IPT and IPT-ACX. Right: the corresponding timings on our multi-core CPU. \relax }{figure.caption.70}{}}
\newlabel{water@cref}{{[figure][3][]3}{[1][12][]12}}
\citation{morgan2000preconditioning}
\citation{morgan2000preconditioning}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces  IPT vs. PRIMME preconditioned eigensolvers for the FCI computation of the ground state of the $5000\times 5000$ tri-diagonal, symmetric matrix considered in \cite  {morgan2000preconditioning}. Left: convergence history, i.e. norm of the residual vector $r = \mathbf  {f}(\mathbf  {z}) - \mathbf  {z}$ vs. matvecs, showing that IPT-ACX requires as few matvecs as the best PRIMME eigensolver (in this case LOBPCG). Right: the corresponding timings on our multi-core CPU. \relax }}{13}{figure.caption.71}\protected@file@percent }
\newlabel{morgan}{{4}{13}{IPT vs. PRIMME preconditioned eigensolvers for the FCI computation of the ground state of the $5000\times 5000$ tri-diagonal, symmetric matrix considered in \cite {morgan2000preconditioning}. Left: convergence history, i.e. norm of the residual vector $r = \mathbf {f}(\mathbf {z}) - \mathbf {z}$ vs. matvecs, showing that IPT-ACX requires as few matvecs as the best PRIMME eigensolver (in this case LOBPCG). Right: the corresponding timings on our multi-core CPU. \relax }{figure.caption.71}{}}
\newlabel{morgan@cref}{{[figure][4][]4}{[1][13][]13}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  Time to compute the $k$ lowest eigenvalues of the tridiagonal Morgan matrix using IPT(-ACX) and PRIMME iterative eigensolvers. Thanks to IPT's perfect parallelism, the larger $k$, the larger its advantage over other methods which target extremal eigenvalues and progress inwards by deflation. \relax }}{13}{figure.caption.72}\protected@file@percent }
\newlabel{num_ev}{{5}{13}{Time to compute the $k$ lowest eigenvalues of the tridiagonal Morgan matrix using IPT(-ACX) and PRIMME iterative eigensolvers. Thanks to IPT's perfect parallelism, the larger $k$, the larger its advantage over other methods which target extremal eigenvalues and progress inwards by deflation. \relax }{figure.caption.72}{}}
\newlabel{num_ev@cref}{{[figure][5][]5}{[1][13][]13}}
\citation{c1998}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  Time to compute the complete eigendecomposition of a dense, non-symmetric matrix with random entries scaled by $\varepsilon = 0.01$ compared to the time for one matrix-matrix multiplication (BLAS 3). Because IPT is perfectly parallel, this ratio hardly increases with the number of CPU cores used, in stark contrast with GEEV. \relax }}{14}{figure.caption.75}\protected@file@percent }
\newlabel{parallel}{{6}{14}{Time to compute the complete eigendecomposition of a dense, non-symmetric matrix with random entries scaled by $\varepsilon = 0.01$ compared to the time for one matrix-matrix multiplication (BLAS 3). Because IPT is perfectly parallel, this ratio hardly increases with the number of CPU cores used, in stark contrast with GEEV. \relax }{figure.caption.75}{}}
\newlabel{parallel@cref}{{[figure][6][]6}{[1][14][]14}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Full spectrum}{14}{subsection.73}\protected@file@percent }
\newlabel{dense_matrix}{{6.1}{14}{Full spectrum}{equation.74}{}}
\newlabel{dense_matrix@cref}{{[equation][1][6]6.1}{[1][14][]14}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  IPT and IPT-ACX convergence histories for a $5000\times 5000$ tri-diagonal matrix with off-diagonal element $\varepsilon $. As the latter increases beyond $\simeq 0.5$, IPT (crosses) ceases to converge; IPT-ACX (circles) allows for larger $\varepsilon $ but requires increasingly many matvecs. For $\varepsilon \gtrsim 15$, IPT-ACX also diverges. \relax }}{15}{figure.caption.76}\protected@file@percent }
\newlabel{fail}{{7}{15}{IPT and IPT-ACX convergence histories for a $5000\times 5000$ tri-diagonal matrix with off-diagonal element $\varepsilon $. As the latter increases beyond $\simeq 0.5$, IPT (crosses) ceases to converge; IPT-ACX (circles) allows for larger $\varepsilon $ but requires increasingly many matvecs. For $\varepsilon \gtrsim 15$, IPT-ACX also diverges. \relax }{figure.caption.76}{}}
\newlabel{fail@cref}{{[figure][7][]7}{[1][15][]15}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Discussion}{15}{section.77}\protected@file@percent }
\citation{Morgan_2009}
\citation{Surj_n_2004}
\bibstyle{ieeetr}
\bibdata{biblio.bib}
\bibcite{rayleigh1894}{1}
\bibcite{kato1995}{2}
\bibcite{pozar1998}{3}
\bibcite{ostlund1996}{4}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Time for the complete diagonalization of random matrices of the form \cref  {dense_matrix} with $\varepsilon = 0.01$ and increasing size $n$. Continuous (resp. dashed) lines correspond to computations on the CPU (resp. GPU) using LAPACK (resp. CUSOLVER). The largest speed-up is obtained for non-symmetric matrices. \relax }}{16}{figure.caption.78}\protected@file@percent }
\newlabel{timings}{{8}{16}{Time for the complete diagonalization of random matrices of the form \cref {dense_matrix} with $\varepsilon = 0.01$ and increasing size $n$. Continuous (resp. dashed) lines correspond to computations on the CPU (resp. GPU) using LAPACK (resp. CUSOLVER). The largest speed-up is obtained for non-symmetric matrices. \relax }{figure.caption.78}{}}
\newlabel{timings@cref}{{[figure][8][]8}{[1][15][]16}}
\@writefile{toc}{\contentsline {section}{References}{16}{section*.79}\protected@file@percent }
\bibcite{demmel1997}{5}
\bibcite{Davidson_1975}{6}
\bibcite{Morgan_2009}{7}
\bibcite{fokkema1998jacobi}{8}
\bibcite{sleijpen2000jacobi}{9}
\bibcite{knyazev2017recent}{10}
\bibcite{Surj_n_2004}{11}
\bibcite{johnson2012}{12}
\bibcite{May_1976}{13}
\bibcite{eyert1996comparative}{14}
\bibcite{Walker_2011}{15}
\bibcite{lepage2021alternating}{16}
\bibcite{stathopoulos2010primme}{17}
\bibcite{hernandez2005slepc}{18}
\bibcite{Julia-2017}{19}
\bibcite{IPT-code}{20}
\bibcite{paper-code}{21}
\bibcite{PYSCF}{22}
\bibcite{morgan2000preconditioning}{23}
\bibcite{c1998}{24}
\bibcite{Lax2007}{25}
\bibcite{Roth_2010}{26}
\citation{Lax2007}
\citation{Roth_2010}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  {}A. Additional lemmas for Section\nobreakspace  {}\ref  {convergence}}{18}{appendix.80}\protected@file@percent }
\newlabel{appendix:full_rank}{{A}{18}{Acknowledgments}{appendix.80}{}}
\newlabel{appendix:full_rank@cref}{{[appendix][1][2147483647]A}{[1][18][]18}}
\@writefile{thm}{\contentsline {lemma}{{Lemma}{A.1}{}}{18}{theorem.81}\protected@file@percent }
\newlabel{lemma-eigenspace}{{A.1}{18}{Acknowledgments}{theorem.81}{}}
\newlabel{lemma-eigenspace@cref}{{[lemma][1][2147483647,1]A.1}{[1][18][]18}}
\@writefile{thm}{\contentsline {proof}{{Proof}{5}{}}{18}{proof.82}\protected@file@percent }
\@writefile{thm}{\contentsline {lemma}{{Lemma}{A.2}{}}{18}{theorem.83}\protected@file@percent }
\newlabel{lemma-defective}{{A.2}{18}{Acknowledgments}{theorem.83}{}}
\newlabel{lemma-defective@cref}{{[lemma][2][2147483647,1]A.2}{[1][18][]18}}
\@writefile{thm}{\contentsline {proof}{{Proof}{6}{}}{18}{proof.84}\protected@file@percent }
\@writefile{thm}{\contentsline {theorem}{{Theorem}{A.3}{\cite {Lax2007} Theorem 8, page 130}}{18}{theorem.85}\protected@file@percent }
\newlabel{theorem-continuous}{{A.3}{18}{Acknowledgments}{theorem.85}{}}
\newlabel{theorem-continuous@cref}{{[theorem][3][2147483647,1]A.3}{[1][18][]18}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  {}B. Rayleigh-Schr\"odinger perturbation theory}{18}{appendix.86}\protected@file@percent }
\newlabel{appendix:RSPT}{{B}{18}{Acknowledgments}{appendix.86}{}}
\newlabel{appendix:RSPT@cref}{{[appendix][2][2147483647]B}{[1][18][]18}}
\citation{kato1995}
\newlabel{ordering}{{B.1}{19}{Acknowledgments}{equation.87}{}}
\newlabel{ordering@cref}{{[equation][1][2147483647,2]B.1}{[1][19][]19}}
\newlabel{power-series}{{B.2}{19}{Acknowledgments}{equation.88}{}}
\newlabel{power-series@cref}{{[equation][2][2147483647,2]B.2}{[1][19][]19}}
\newlabel{A-series}{{B.3}{19}{Acknowledgments}{equation.89}{}}
\newlabel{A-series@cref}{{[equation][3][2147483647,2]B.3}{[1][19][]19}}
\newlabel{RS-recursion}{{B.4}{19}{Acknowledgments}{equation.90}{}}
\newlabel{RS-recursion@cref}{{[equation][4][2147483647,2]B.4}{[1][19][]19}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  {}C. Iterative perturbation theory contains the RS series}{20}{appendix.91}\protected@file@percent }
\newlabel{appendix:IPT_RSPT}{{C}{20}{Acknowledgments}{appendix.91}{}}
\newlabel{appendix:IPT_RSPT@cref}{{[appendix][3][2147483647]C}{[1][19][]20}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  {}D. Explicit examples}{20}{appendix.92}\protected@file@percent }
\newlabel{appendix:examples}{{D}{20}{Acknowledgments}{appendix.92}{}}
\newlabel{appendix:examples@cref}{{[appendix][4][2147483647]D}{[1][20][]20}}
\newlabel{further-examples}{{D}{20}{Acknowledgments}{appendix.92}{}}
\newlabel{further-examples@cref}{{[appendix][4][2147483647]D}{[1][20][]20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.1}A general approach to find the convergence domain}{20}{subsection.93}\protected@file@percent }
\newlabel{system-one}{{D.1}{20}{A general approach to find the convergence domain}{subsection.93}{}}
\newlabel{system-one@cref}{{[subappendix][1][2147483647,4]D.1}{[1][20][]20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {D.2}The $2\times 2$ example of the main text}{21}{subsection.94}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {D.3}An additional $3\times 3$ example}{21}{subsection.95}\protected@file@percent }
\newlabel{poly-n1}{{D.1}{22}{An additional $3\times 3$ example}{equation.96}{}}
\newlabel{poly-n1@cref}{{[equation][1][2147483647,4]D.1}{[1][21][]22}}
\newlabel{poly-n2}{{D.2}{22}{An additional $3\times 3$ example}{equation.97}{}}
\newlabel{poly-n2@cref}{{[equation][2][2147483647,4]D.2}{[1][22][]22}}
\newlabel{poly-n3}{{D.3}{22}{An additional $3\times 3$ example}{equation.98}{}}
\newlabel{poly-n3@cref}{{[equation][3][2147483647,4]D.3}{[1][22][]22}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces {The convergence domain on the\nobreakspace  {}\(\varepsilon \)-plane for the first column of\nobreakspace  {}\(Z\)\nobreakspace  {}(the first eigenvector \(\mathbf  z_1\)) for the\nobreakspace  {}\(3\times 3\) example. The Mandelbrot-like set (domain where orbits remain bounded) of the iterative scheme is shown in black and grey. The empirical convergence domain is shown in black. Its largest component corresponds to the stability of a steady state (the applicability domain of the iterative method). Small components correspond to stability of various periodic orbits. Various shades of grey show the values of $\varepsilon $ that lead to divergence to infinity (the darker the slower the divergence). In red are the values of $\varepsilon $ where the matrix is non-diagonalizable. {}}\relax }}{24}{figure.caption.99}\protected@file@percent }
\newlabel{473965}{{9}{24}{{The convergence domain on the~\(\parameter \)-plane for the first column of~\(\Z \)~(the first eigenvector \(\mathbf z_1\)) for the~\(3\times 3\) example. The Mandelbrot-like set (domain where orbits remain bounded) of the iterative scheme is shown in black and grey. The empirical convergence domain is shown in black. Its largest component corresponds to the stability of a steady state (the applicability domain of the iterative method). Small components correspond to stability of various periodic orbits. Various shades of grey show the values of $\parameter $ that lead to divergence to infinity (the darker the slower the divergence). In red are the values of $\parameter $ where the matrix is non-diagonalizable. {\label {473965}}}\relax }{figure.caption.99}{}}
\newlabel{473965@cref}{{[figure][9][2147483647]9}{[1][24][]24}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces {{Same as \nobreakspace  {}{\cref  {473965}} for the second eigenvector\nobreakspace  {}\(\mathbf  z_2\) (the second column of \(Z\)). Small components correspond to stability of various periodic orbits. Various shades of grey show the values of\nobreakspace  {}\(\varepsilon \) that lead to divergence to infinity (the darker the slower the divergence). {}}}\relax }}{25}{figure.caption.100}\protected@file@percent }
\newlabel{988969}{{10}{25}{{{Same as ~{\cref {473965}} for the second eigenvector~\(\mathbf z_2\) (the second column of \(\Z \)). Small components correspond to stability of various periodic orbits. Various shades of grey show the values of~\(\parameter \) that lead to divergence to infinity (the darker the slower the divergence). {\label {988969}}}}\relax }{figure.caption.100}{}}
\newlabel{988969@cref}{{[figure][10][2147483647]10}{[1][24][]25}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces {{Same as \nobreakspace  {}{\cref  {473965}} for the third eigenvector\nobreakspace  {}\(\mathbf  z_3\) (the third column of\nobreakspace  {}\(Z\)) . {}}}\relax }}{26}{figure.caption.101}\protected@file@percent }
\newlabel{824432}{{11}{26}{{{Same as ~{\cref {473965}} for the third eigenvector~\(\mathbf z_3\) (the third column of~\(\Z \)) . {\label {824432}}}}\relax }{figure.caption.101}{}}
\newlabel{824432@cref}{{[figure][11][2147483647]11}{[1][24][]26}}
\@writefile{toc}{\contentsline {section}{Appendix\nobreakspace  {}E. Exceptional points and cusps of $C$}{26}{appendix.102}\protected@file@percent }
\newlabel{appendix:cusps}{{E}{26}{An additional $3\times 3$ example}{appendix.102}{}}
\newlabel{appendix:cusps@cref}{{[appendix][5][2147483647]E}{[1][26][]26}}
\@writefile{thm}{\contentsline {theorem}{{Theorem}{E.1}{}}{27}{theorem.103}\protected@file@percent }
\@writefile{thm}{\contentsline {proof}{{Proof}{7}{}}{27}{proof.104}\protected@file@percent }
\newlabel{eq-nu}{{E.1}{27}{An additional $3\times 3$ example}{equation.105}{}}
\newlabel{eq-nu@cref}{{[equation][1][2147483647,5]E.1}{[1][27][]27}}
\gdef \@abspage@last{28}
