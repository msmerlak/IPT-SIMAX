% LaTeX rebuttal letter example. 
% 
% Copyright 2019 Friedemann Zenke, fzenke.net
%
% Based on examples by Dirk Eddelbuettel, Fran and others from 
% https://tex.stackexchange.com/questions/2317/latex-style-or-macro-for-detailed-response-to-referee-report
% 
% Licensed under cc by-sa 3.0 with attribution required.

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{lipsum} % to generate some filler text
\usepackage{fullpage}

% import Eq and Section references from the main manuscript where needed
% \usepackage{xr}
% \externaldocument{manuscript}

% package needed for optional arguments
\usepackage{xifthen}
% define counters for reviewers and their points
\newcounter{reviewer}
\setcounter{reviewer}{0}
\newcounter{point}[reviewer]
\setcounter{point}{0}

% This refines the format of how the reviewer/point reference will appear.
\renewcommand{\thepoint}{P\,\thereviewer.\arabic{point}} 

% command declarations for reviewer points and our responses
\newcommand{\reviewersection}{\stepcounter{reviewer} \bigskip \hrule
                  \section*{Reviewer \thereviewer}}

\newenvironment{point}
   {\refstepcounter{point} \bigskip \noindent {\textbf{Reviewer~Point~\thepoint} } ---\ }
   {\par }

\newcommand{\shortpoint}[1]{\refstepcounter{point}  \bigskip \noindent 
	{\textbf{Reviewer~Point~\thepoint} } ---~#1\par }

\newenvironment{reply}
   {\medskip \noindent \begin{sf}\textbf{Reply}:\  }
   {\medskip \end{sf}}

\newcommand{\shortreply}[2][]{\medskip \noindent \begin{sf}\textbf{Reply}:\  #2
	\ifthenelse{\equal{#1}{}}{}{ \hfill \footnotesize (#1)}%
	\medskip \end{sf}}

\begin{document}

\section*{Response to the reviewers}

We are glad to thank the reviewers for their constructive, clarifying comments. We now resubmit our paper---with a long delay for which we apologize---with several substantial modifications. 

\begin{itemize}
    \item Neither of the referees commented on what we feel are the most interesting aspects of our work, namely: (1) IPT is both a `direct' and an `iterative' eigensolver, in that it computes any number of eigenvalues with the same basic iterative procedure; (2) IPT is elementary (a dozen lines of code) and self-contained: it does not rely on Hessenberg reduction, does not use the Rayleigh-Ritz procedure, etc. We have rewritten the abstract and introduction in order to emphasize these points more. 
    \item The editor and referees asked for less geometry and more numerical linear algebra. We have cut (or pushed to the appendix) several sections, including those dealing with cusps and exceptional points. We hope the paper is more easily readable for this community now. The little geometry we left, we feel is useful for two reasons: it provides intuition for why the algorithm should work (which guided us to our proofs), and provide useful in the future to extend the method further (see Discussion). 
    \item We found that another software package, PRIMME, is both friendlier to the user and faster than SLEPc (at least on our single-node system). We now compare IPT (in a new Julia implementation, faster than the original Python) to PRIMME rather than SLEPc.
    \item The section on iterative refinement was removed: we feel it did not add much (and neither of the referees commented on it). We also streamlined the quantum chemistry example. 
\end{itemize}

In the following we address the referees' concerns point by point. 


\reviewersection

\begin{point}
    The technical sections 2 and 3 are still quite difficult to follow without experience using charts, projective varieties, cusps etc. 
    This make me wonder if the matrix-oriented readership of SIMAX will fully appreciate the work. (e.g., I'm not sure why the whole idea couldn't be formulated in affine coordinates alone)
\end{point}


\begin{reply}
We have cut much of these sections and hope this is no longer a concern.
\end{reply}

\begin{point}
    This should be put in perspective. One would never solve the linear systems in JD / GD exactly (i.e. by direct methods). Those are handled by a few steps of an iterative (preconditioned Krylov subspace) solver, so only some matrix vector products and maybe preconditioner applications are needed. JD with exact solves is equivalent to Rayleigh quotient iteration.
\end{point}


\begin{reply}
    This is true, the approximate solve steps only involve a few more matvecs. They nevertheless increase runtime with respect to IPT, as the new Fig. 3-4 show. 
\end{reply}

\begin{point}
    Were just standard settings from Slepc used? A short explanation on that would improve the comparison. 
\end{point}


\begin{reply}
    As noted we now use PRIMME rather than SLEPc for reference iterative solvers. (We found PRIMME to be faster, and easier to extract convergence histories.) As explained in sec. 6, we use PRIMME algorithms (LOBPCG, GD, JDQMR, etc) with $D^{-1}$ as preconditioner. Other than this, parameters have their default values. 
\end{reply}

\reviewersection

\begin{point}

The statement that this iteration (which iterates all the eigenvectors at the same time) produces a full-rank matrix is highly non-trivial and constitutes the main theoretical result of the paper (given in the Appendix). For some reason, it is formulated as a Lemma.

\end{point}

\begin{reply}
    Thank you. We have renamed this result a ``theorem". 
\end{reply}

\begin{point}
    The section 4.4. "Cusps and exceptional points" does not give any specific results (I may be missing something), thus it is better to provide explicit statements of what has been done in this section and what insights are obtained. 
\end{point}

\begin{reply}
    We have moved this discussion to the appendix under "Explicit examples". 
\end{reply}


\begin{point}
    However, the class of matrices for which the proposed method is guaranteed to work is not so big, so examples and tests on matrices that are not "good" perturbations of the the diagonal should be added. What happens if the condition is violated? 
\end{point}

\begin{reply}
    We have added a figure (Fig. 6) to address this question. In short, as off-diagonal elements grow, the number of iterations required for convergence also grows---until the iteration diverges. Fig. 2 also addresses this question from a different perspective, namely by comparison with Rayleigh-Schr\"odinger theory. 
\end{reply}

\begin{point}
    It is not clear, how the proposed method is used to find the lowest lying eigenspace. 
\end{point}

\begin{reply}
    The method does not single out extremal eigenvalues the way standard iterative solvers do. To get the lowest eigenvalue, we simply consider the column $i$ of $M$ such that $M_{ii} = \min\textrm{diag}(M)$. We have added a sentence in sec. 6.1 to this effect.  
\end{reply}

\begin{point}
The details of hyperparameters of other methods are also not included (i.e. block size, etc.) 
\end{point}

\begin{reply}
    We now use PRIMME rather than SLEPc with $D^{-1}$ as preconditioner and default settings otherwise. A sentence in sec. 6.1 states this explicitly. 
\end{reply}

\begin{point}
    It would be interesting to see the convergence curve for the IPT method with respect to the number of iterations. 
\end{point}
\begin{reply}
    Please see the left panels of Fig. 3 and 4.  
\end{reply}

\begin{point}
    Jacobi-Davidson method is typically more efficient that Davidson alone
\end{point}
\begin{reply}
    Not necessarily when $M$ is near-diagonal, as Fig. 3 and 4 show. In quantum chemistry applications, Davidson is used more commonly than JD. In any case, we now compare IPT to both GD, JD, and some combination of them ("PRIMME-DYNAMIC"). 
\end{reply}
\begin{point}
    With exact solve, Jacobi-Davidson is a Riemannian Newton method, i.e. it enjoys quadratic convergence. If the matrix is close to the diagonal, I would expect linear convergence with very small factor. Is it confirmed by experiments (this can be verified by looking at the convergence curve, not only the final iteration). 
\end{point}
\begin{reply}
   RQI is now included in performance comparisons. As anticipated by the referee, RQI requires very few iterations for a near-diagonal matrix, but since each iteration is more expansive, RQI is slower than GD or JD. 
\end{reply}

\reviewersection

\begin{point}
    I understand that the authors have invested a lot of time into developing these ideas and writing the paper. The second version of the paper is better than the first version, but I believe that these methods will have very little impact. I suggest publishing the paper in a lower quality journal. 
\end{point}
\begin{reply}
    Any specific recommendations? We would love to have a list of low-quality journals for our future use. 
\end{reply}

\begin{point}
    In their response to the referees the authors state that it could also be used to compute a block of eigenvalues. If that can be implemented and the method is competitive then the paper would probably be publishable in SIAM Journal on Matrix Analysis and Applications.  
\end{point}

\begin{reply}
    Indeed it can, please see Fig. 5. 
\end{reply}


\begin{point}
    I continue to believe that the niche in which this method is useful is tiny. In their response the authors mention their section 6.1. However, transforming M into the Z0 basis is itself costly. I do not think this is useful.
\end{point}

\begin{reply}
    We have removed this section altogether. 
\end{reply}

\begin{point}
    From Figure 6, I infer that for symmetric dense matrices GD is better unless epsilon is extremely small
\end{point}

\begin{reply}
    IPT is comparable to GD for dense symmetric matrices when one eigenpair is requested, but much faster when $5$ or more are requested due to its parallelism (Fig. 5). For instance, for a dense symmetric matrix with $\epsilon = .1$, IPT computes $k = 10$ eigenpairs in $50$ms (the time required for $18$ matvecs); PRIMME's GD by contrast needs $400$ms (the time for $80$ matvecs).
\end{reply}

\begin{point}
    Table 1 shows that "IPT is equally efficient as the Davidson algorithm". This confirms what I wrote in my first report "I see no reason to believe that this method is in some sense better". LOBPCG and GD might be even better than the Davidson used to make table 1. 
\end{point}

\begin{reply}
    Some reasons why our method is at least in some sense better include:  its higher parallelism, the fact that one or all eigenvalues can be computed using the exact same method (i.e. IPT is both `direct' and `iterative'), its non-reliance on Rayleigh-Ritz. And, most important in our view, its elementary character. There is nothing fancy in IPT---just a straightforward fixed-point iteration. 
\end{reply}

\end{document}