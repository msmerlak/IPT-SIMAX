% LaTeX rebuttal letter example. 
% 
% Copyright 2019 Friedemann Zenke, fzenke.net
%
% Based on examples by Dirk Eddelbuettel, Fran and others from 
% https://tex.stackexchange.com/questions/2317/latex-style-or-macro-for-detailed-response-to-referee-report
% 
% Licensed under cc by-sa 3.0 with attribution required.

\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage{lipsum} % to generate some filler text
\usepackage{fullpage}

% import Eq and Section references from the main manuscript where needed
% \usepackage{xr}
% \externaldocument{manuscript}

% package needed for optional arguments
\usepackage{xifthen}
% define counters for reviewers and their points
\newcounter{reviewer}
\setcounter{reviewer}{0}
\newcounter{point}[reviewer]
\setcounter{point}{0}

% This refines the format of how the reviewer/point reference will appear.
\renewcommand{\thepoint}{P\,\thereviewer.\arabic{point}} 

% command declarations for reviewer points and our responses
\newcommand{\reviewersection}{\stepcounter{reviewer} \bigskip \hrule
                  \section*{Reviewer \thereviewer}}

\newenvironment{point}
   {\refstepcounter{point} \bigskip \noindent {\textbf{Reviewer~Point~\thepoint} } ---\ }
   {\par }

\newcommand{\shortpoint}[1]{\refstepcounter{point}  \bigskip \noindent 
	{\textbf{Reviewer~Point~\thepoint} } ---~#1\par }

\newenvironment{reply}
   {\medskip \noindent \begin{sf}\textbf{Reply}:\  }
   {\medskip \end{sf}}

\newcommand{\shortreply}[2][]{\medskip \noindent \begin{sf}\textbf{Reply}:\  #2
	\ifthenelse{\equal{#1}{}}{}{ \hfill \footnotesize (#1)}%
	\medskip \end{sf}}

\begin{document}

\section*{Response to the reviewers}
% General intro text goes here
We thank the reviewers for their assessment of our work. The main criticism is that IPT was only compared to Davidson and Krylov-Schur, and not to other methods such as LOBPCG, Jacobi-Davison, ``improved Davidson-type" algorithms, etc. We note two points:

\begin{itemize}
    \item Our approach provides a unified treatment of single-eigenpair and full-spectrum near-diagonal eigenproblems. The above criticism only bears on the latter. 
    \item Our new benchmark strategy for single-eigenpair problems uses SLEPc, which provides modern implementations of Krylov-Schur, but also Generalized Davidson (GD), Jacobi-Davidson (JD) and, in the symmetric case, LOBPCG.
\end{itemize}

In the following we address the referees' concerns point by point. 

% Let's start point-by-point with Reviewer 1
\reviewersection

% Point one description 
\begin{point}
Page 1. It should be clarified, why Davidson methods can be faster than Lanczos. After all, modern versions of Lanczos form the basis of modern state-of-the-art solvers for symmetric eigenvalue problem. Moreover, Jacobi-Davidson methods 
should be mentioned, referenced, and put in the comparison here. See, e.g., 
\begin{itemize} 
\item G.L.G. Sleijpen, H.A. van der Vorst: A Jacobi-Davidson iteration method for linear eigenvalue problems. 
SIAM Review 42(2), pp. 267-293, 2000. 
\item D.R. Fokkema, G.L.G. Sleijpen, H.A. van der Vorst: Jacobi-Davidson style QR and QZ algorithms for the reduction of matrix pencils. SIAM J. Sci. Comput. 20(1), pp. 94-125, 1998. 
\end{itemize} 
\end{point}

% Our reply
\begin{reply}
The performance of Davidson vs Lanczos for near-diagonal problems has been been discussed in the literature, see e.g. 
\begin{itemize}
    \item Morgan, Ronald B., and David S. Scott. "Generalizations of Davidsonâ€™s method for computing eigenvalues of sparse symmetric matrices." SIAM Journal on Scientific and Statistical Computing 7.3 (1986): 817-825.
\end{itemize}
We see no reason to repeat this discussion here, referring instead the reader to (Morgan, Scott 86) above (sec. 5.2, first paragraph). 

\end{reply}


\begin{point}
Page 2. "`While more restricted in its applicability 
that Davidson- or Jacobi-like methods, its performance is higher. .... simple linear-algebraic structure: each iteration consists of just one 
matrix-vector multiplication ..."': Please put this in the right perspective. Lanzcos, (Jacobi-)Davidson methods also consist of little more than matrix-vector products if cleverly implemented (maybe also on GPUs). So from this standpoint I don't see why IPT should be much faster.
\end{point}
\begin{reply}
It all depends on the number of iterations required for convergence. What our results show is that, for near-diagonal problems of the kind discussed in the paper, IPT requires fewer (and sometimes many fewer) iterations than Lanczos or Davidson. Please note that Jacobi-Davidson involves solving linear systems at each iteration, which comes on top of matrix-vector product steps and increases runtime.  
\end{reply}


\begin{point}
Page 2. it is at this point not clear what "`logistic map"' and "`period-doubling route to chaos"' means 
\end{point}

\begin{reply}
We have changed the wording and explicitly explained what we had meant by the period-doubling route to chaos.
\end{reply}

\begin{point}
Page 3. Lemma 2.1.: I couldn't find the meaning of the notation $Mz\wedge z$. 
\end{point}

\begin{reply}
By the wedge product $M\mathbf{z}\wedge \mathbf{z}$ we simply meant the antisymmetrized tensor product $(M\mathbf{z}\otimes\mathbf{z} - \mathbf{z}\otimes M\mathbf{z})/2$. We have changed the text removing unnecessary complications at that place of the manuscript.
\end{reply}

\begin{point}
Page 3. the remaining text is very hard to understand here: especially the paragraph starting with "Being defined up to .." and the last paragraph of this page. This might need some clarification. Maybe a small illustrative example helps. Also explain what "`atlas of affine charts"' means. 
\end{point}

\begin{reply}
To facilitate understanding of the concepts used in that place of the manuscript, we have added a new section \emph{Definitions and notations}. We have also added a new sketch (Figure 1) to aid intuition.
\end{reply}

\begin{point}
Page 4. Algorithm 2.1.: what is used a stopping criterion? This is also crucial for the later numerical experiments.
\end{point}
\begin{reply}
Given a user-specified tolerance $\eta$, the stopping criterion is $\vert\mathbf z - \mathbf{f}_i(\mathbf z)\vert/\vert\mathbf z\vert>\eta$ (resp. $\Vert Z -F(Z)\Vert/\Vert Z\Vert > \eta $) for the one-eigenpair (resp. full-spectrum) problem. We have made this explicit in Algorithms 3.1 and 3.2. 

In the numerical experiments of sec. 4, we used $\eta = 100\epsilon$, where $\epsilon\simeq 2.2 \cdot 10^{-16}$ is machine epsilon. We added a sentence in section 5 to clarify this point. 
\end{reply}


\begin{point}
Page 5. $A^*$ not defined 
\end{point}

\begin{reply}
$A^*$ is first used in the proof of Theorem~3.1, where it is well defined (the unique fixed point provided by the Banach fixed-point theorem). It is later used in the formulation and in the proof of Lemma~3.2, where it is defined, too. The usage of $A^*$ is sloppy in Lemma~A.1 and Lemma~A.2. We have corrected this.
\end{reply}

\begin{point}
Sections 3.2-3.4 are difficult to grasp since they contain a lot of concepts from dynamical systems theory which might not be familiar to everyone. 
\end{point}

\begin{reply}
We have a sentence at the end of the section now labelled 4.1 to the effect that sec. 4.2-4.4 may be omitted by the non-specialist reader.  
\end{reply}

\begin{point}
Pages 9--10. Figure 2: to which accuracy are eigenpairs computed here / what is the stopping criterion for IPT? Comparing computation times means little if the obtained accuracy from different methods is very different. This problem is also found in several later experiments. 
\end{point}

\begin{reply}
In all cases we used an accuracy of $10\epsilon$. We have added a sentence to that effect in the first paragraph of section 5.
\end{reply}


\begin{point}
Page 10. "`This is another difference with standard full spectrum algorithms, which do not take 
advantage of sparsity."' Note that there are nowadays full spectrum algorithms available for symmetric EVP and the SVD.
\end{point}

\begin{reply}
We do not understand this comment. That there are "full spectrum algorithms available for symmetric EVP and the SVD" is not in question. Our point is that existing algorithms do not take advantage of sparsity. The shifted QR algorithm, for instance, involves a reduction to tridiagonal form which breaks sparsity.  
\end{reply}

\begin{point}
Page 11. Notation (was maybe introduced earlier): if $z_k$ denote the candidate vectors / iterates / eigenvectors, why not denote the matrix that containing those as columns by $Z_k$ instead of $A$? Also some further small changes in the notation would make some things already easier to digest, e.g., why not denote the eigenvalues $\lambda$ as it is customary? I admit that this does not alter anything but the name of variables, but here $\lambda$ is used for something else which leads to some confusion. 
\end{point}

\begin{reply}
We have decided to follow this recommendation and throughout the text we have changed the notation of the eigenvector matrix form $A$ to $Z$ (the matrix built of vectors $\mathbf{z}_k$), the notation of the eigenvalues form $\varepsilon$ to more traditional $\lambda$, and the notation of the perturbation parameter from $\lambda$ to $\varepsilon$.
\end{reply}

\begin{point}
Page 13. The comparison with Lanczos and Arnoldi is in part not fair: Lanczos, Arnoldi were intrinsically designed to (1) compute a few eigenpairs and \underline{not the full spectrum} and (2) work with sparse and \underline{not dense matrices}. So it is little wonder that they don't compete well for full spectrum computations and/or dense matrices. 
\end{point}
\begin{reply}
We do not understand this comment. Nowhere do we compare IPT with Lanczos or Arnoldi on full spectrum problems. As noted by the referee, this would be meaningless, as these algorithms are designed to compute a few extremal eigenvectors. 
\end{reply}


\begin{point}
 Page 5. "`such that the corresponding to $D$ matrix of inverse gaps $G$ is defined, which implies ...... of $D$ are \textit{pare-wise} different"'. There's something wrong with the sentence here. 
\end{point}

\begin{reply}
It was a typo. We meant \textit{pair-wise}. The text has been corrected and now uses \emph{pairwise} consistently with the main part of the manuscript.
\end{reply}

\begin{point}
Page 6. "`Then the rank can be lost only by 
a \textit{repeat} of an eigenvector"'. Maybe better: repetition
\end{point}

\begin{reply}
We have corrected the word.
\end{reply}

\reviewersection

\begin{point}
I see nothing wrong with this paper. It is possible that for small matrices with very small off-diagonal elements this is a useful method. That seems like a small niche to me.
\end{point}
\begin{reply}
This is a fair point, although application do exist, e.g. in quantum chemistry, as noted in the paper. Another possible application is for the iterative refinement of low-precision diagonalization routines, as in sec. 6.1.
\end{reply}

\begin{point}
For large matrices there are well established methods for finding the smallest or a set of the smallest eigenvalues. The method of this paper is compared only with Davidson. There is no mention of methods like LOBPCG (with or without a block) and no mention of simple Lanczos methods (with or without orthogonalization). 
\end{point}

\begin{reply}
We have modified our benchmark strategy for single-eigenpair computations. We now compare IPT to methods in the SLEPc suite: Krylov-Schur, GD, JD, and (for symmetric problems) LOBPCG. 
\end{reply}

\begin{point}
I see no reason to believe that this method is in some sense better.
\end{point}

\begin{reply}
Please see sec. 5 for evidence.  
\end{reply}

\begin{point}
 There is also no mention of shift and invert methods. It is compared only to Davidson and according to the results of the paper, it is not really better than standard Davidson. There are also various improved Davidson-type algorithms.
 \end{point}
 
 \begin{reply}
 Shift-and-inverse methods do not improve convergence in the cases treated here. As noted, we have added a comparison to GD and JD with diagonal preconditioners. 
 \end{reply}
 
 \begin{point}
 The method might be more useful if it could be used to get not one eigenvalue and not all the eigenvalues but a block of eigenvalues. 
 \end{point}
 
 \begin{reply}
 It can: instead of forming a square matrix containing all candidate eigenvectors and iterating $F$, we could select a subset of them and iterate on a rectangular matrix. We added remarks to this effect before eq. (2.2).  
 \end{reply}
 
 \begin{point}
 I'd like to see more steps in the derivation of 2.1 from Lemma 2.1. 
 \end{point}
 
 \begin{reply}
 We have written the implied steps explicitly. We also added a new section \emph{Definitions and notations} and a new sketch (Figure 1) to facilitate understanding of the concepts employed in the manuscript.
 \end{reply}
 
 \begin{point}
 In the figures, it would be nice to know how many IPT iterations are required. 
 \end{point}
 \begin{reply}
 The figures plot timings relative to matrix-matrix or matrix-vector multiplications, hence the values in these figures can be read as numbers of IPT iterations. 
 \end{reply}
 
 
 \begin{point}
 The ideas seem interesting to me, but the authors should be required to show that they are useful. 
\end{point}
\begin{reply}
Please see sec. 6 to this effect. 
\end{reply}

\end{document}